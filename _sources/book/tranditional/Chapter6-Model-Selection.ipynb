{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We Need Model Selection\n",
    "\n",
    "- Model selection is essential because it helps us choose the most appropriate model among a set of candidate models.\n",
    "- In the case of linear models, such as linear regression, there are often multiple ways to specify the model, including\n",
    "  - Different combinations of predictors, polynomial terms, and interaction terms\n",
    "- Selecting the best model ensures that our model performs well on unseen data and avoids `overfitting` or `underfitting`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three classes of methods\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "- Subset selection involves identifying and selecting a subset of predictors from a larger pool of variables.\n",
    "- Model selection should use $C_p$, $AIC$, $BIC$ or `deviance`. **Using $RSS$ or $R^2$ is inappropriate**\n",
    "  - Deviance: negative two times the maximized log-likelihood\n",
    "- There are two main approaches to subset selection:\n",
    "\n",
    "  - Best Subset selection\n",
    "    - Computational expensive\n",
    "\n",
    "  ![image-3.png](./img/38.png)\n",
    "\n",
    "  - Forward and Backward Stepwise Selection\n",
    "    - Forward and backward stepwise selection are iterative approaches that start with either no predictors (forward selection) or all predictors (backward selection) and sequentially add or remove predictors based on certain criteria until the optimal subset is found.\n",
    "    - Approximate function, do not guarantee the best model\n",
    "\n",
    "  ![image-3.png](./img/39.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating test Error\n",
    "\n",
    "1. We can **indirectly** estimate test error by making an **adjustment** to the training error\n",
    "   - $C_p$, $AIC$, $BIC$, Adjusted $R^2$\n",
    "   - Adjusted $R^2$ = $1-\\frac{RSS/(n-d-1)}{TSS/(n-1)}$\n",
    "     - $d$ is the number of predictors\n",
    "     - Not generalized to other model like logistic regression, not that strong theory support\n",
    "2. We can **directly** estimate the test error using either a `validation` set approach or `cross-validation` approach.\n",
    "\n",
    "### One-standard-error rule\n",
    "\n",
    "1. Calculate the mean and standard error $\\sigma^2$ of the estimated test MSE\n",
    "2. Select the model with smallest $k$ for test MSE within one standard error\n",
    "\n",
    "![image-3.png](./img/40.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage Methods\n",
    "\n",
    "- `Shrinkage` methods, also known as `regularization` techniques, **penalize the coefficients of predictors to discourage `overfitting`** and improve the generalization ability of the model.\n",
    "- Shrinkage methods are computationally efficient and can handle a large number of predictors.\n",
    "  - **Lasso regression**, in particular, performs variable selection by **setting some coefficients to zero**, leading to sparse models.\n",
    "\n",
    "#### Ridge Regression\n",
    "\n",
    "1. Add penalty of coefficients to RSS\n",
    "   $$\n",
    "   RSS = \\sum_{i}^{n}(y_i-\\beta_0-\\sum_{j=1}^{p}\\beta_jx_{ij})\n",
    "   $$\n",
    "2. Ridge Regression want to minimize\n",
    "   - $\\lambda > 0$ is a tuning parameter\n",
    "   - $\\lambda \\sum_{j=1}^{p}\\beta_j^2$ called a shrinkage penalty\n",
    "     - It's small when coefficients are small, it shrinking the estimates of $\\beta_j$ towards zero\n",
    "   - Since the penalty is squared of coefficients, the estimates of ridge regression is scale sensitive\n",
    "     - It's best to apply ridge regression after **standardizing the predictors**\n",
    "   - **Ridge regression push coefficients toward 0 but never set them to 0**\n",
    "     $$\n",
    "     RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2 = \\sum_{i}^{n}(y_i-\\beta_0-\\sum_{j=1}^{p}\\beta_jx_{ij}) + \\lambda \\sum_{j=1}^{p}\\beta_j^2\n",
    "     $$\n",
    "\n",
    "![image-3.png](./img/41.png)\n",
    "\n",
    "![image-3.png](./img/42.png)\n",
    "\n",
    "#### Lasso Regression\n",
    "\n",
    "1. Similarly, add penalty of coefficients to RSS but using absolute value\n",
    "   - In statistical parlance, the lasso use an $l_1$ penalty instead of an $l_2$ penalty.\n",
    "   - $l_1$ has the effect of forcing some of coefficient estimates **to be exactly 0.** Therefore, **lasso yields sparse models**, that is, models that involve only a subset of variables.\n",
    "2. Lasso Regression want to minimize\n",
    "   $$\n",
    "   RSS + \\lambda \\sum_{j=1}^{p}|\\beta_j| = \\sum_{i}^{n}(y_i-\\beta_0-\\sum_{j=1}^{p}\\beta_jx_{ij}) + \\lambda \\sum_{j=1}^{p}|\\beta_j|\n",
    "   $$\n",
    "\n",
    "![image-3.png](./img/43.png)\n",
    "\n",
    "![image-3.png](./img/47.png)\n",
    "\n",
    "### Why lasso gives us the 0 estimates\n",
    "\n",
    "![image-3.png](./img/44.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning of Lasso and Ridge\n",
    "\n",
    "- Since we are push coefficients to 0 to product a spare model, **use $C_p$, $AIC$, $BIC$ might not a good idea**\n",
    "- Use cross-validation provides a simple way to tackle the `unknown number of predictors` problem\n",
    "\n",
    "  ![image-3.png](./img/45.png)\n",
    "\n",
    "  ![image-3.png](./img/46.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
