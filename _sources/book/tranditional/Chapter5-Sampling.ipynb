{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "- Sometimes we need to test the model performance, but we can not use the same data as training and testing. therefore we need sampling strategies.\n",
    "- **Training error** (blue line): Training error, also known as in-sample error, is the error that occurs when a model is applied to the data it was trained on.\n",
    "- **Testing error** (red line): Testing error, or out-of-sample error, is the error that occurs when the model is applied to a new dataset that it hasn't seen before, often a validation or test set.\n",
    "\n",
    "![image-3.png](./img/29.png)\n",
    "\n",
    "## Estimate Testing error\n",
    "\n",
    "- When we use training error to estimate testing error, we often get a **underestimated** result. Some methods make a **mathematical adjustment** to the training error rate in order to estimate the test error rate.\n",
    "  - $-2ln(L)$ use the same way as RSS, in linear regression, $-2log(L) = RSS/\\sigma^2$\n",
    "\n",
    "### AIC (Akaike Information Criterion)\n",
    "\n",
    "$$\n",
    "AIC=2k-2ln(L)\n",
    "$$\n",
    "\n",
    "- $k$ is the number of parameters in the model\n",
    "- $L$ is the maximum value of the likelihood function for the model\n",
    "  - Normally, we assume **errors are normally distributed** (offsets between expectation and observation)\n",
    "\n",
    "### BIC (Bayesian Information Criterion)\n",
    "\n",
    "$$\n",
    "BIC=ln(n)k-2ln(L)\n",
    "$$\n",
    "\n",
    "- $n$ is the number of observations\n",
    "- The other terms are as defined for AIC\n",
    "- when $n > 7$, $BIC$ put more penalty than $AIC$\n",
    "\n",
    "### Example\n",
    "\n",
    "- Want to compare linear vs higher-order polynomial terms in a linear regression\n",
    "\n",
    "![image-3.png](./img/30.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model by AIC is of degree 2 with AIC = 425.7373213865948\n",
      "Best model by BIC is of degree 2 with BIC = 433.55283194455905\n"
     ]
    }
   ],
   "source": [
    "# Generate some data\n",
    "np.random.seed(0)\n",
    "X = np.random.uniform(-10, 10, 100)\n",
    "Y = 3 - 2*X + X**2 + np.random.normal(0, 2, 100)\n",
    "\n",
    "# Fit models of varying complexity\n",
    "results = {}\n",
    "for degree in range(1, 6):\n",
    "    features = np.column_stack([X**i for i in range(1, degree + 1)])\n",
    "    features = sm.add_constant(features)\n",
    "    model = sm.OLS(Y, features).fit()\n",
    "    results[degree] = {'model': model, 'AIC': model.aic, 'BIC': model.bic}\n",
    "\n",
    "# Find the best model according to AIC and BIC\n",
    "best_by_aic = min(results, key=lambda k: results[k]['AIC'])\n",
    "best_by_bic = min(results, key=lambda k: results[k]['BIC'])\n",
    "\n",
    "print(f'Best model by AIC is of degree {best_by_aic} with AIC = {results[best_by_aic][\"AIC\"]}')\n",
    "print(f'Best model by BIC is of degree {best_by_bic} with BIC = {results[best_by_bic][\"BIC\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "- In normal validation approach, **only a subset of the observation are used to fit the model**\n",
    "  - The validation error may tend to **overestimate the test error** for the model fit on the entire data set\n",
    "  - For some shrinkage method (e.g. Lasso and Ridge regression), it's hard to find the number of predictors $k$\n",
    "  - Cross validation provide a direct way to test error, instead of using the variance of error $\\sigma^2$\n",
    "\n",
    "### K-fold Cross Validation\n",
    "\n",
    "- Let the $K$ parts be $C_1,C_2,...,C_K$, where $C_k$ denotes the indices of the observations in part $k$. There're $n_k$ observations in part k, where $n_k=n/K$\n",
    "- Compute (regression)\n",
    "\n",
    "  $$\n",
    "  CV_{(K)}=\\sum_{k=1}^K\\frac{n_k}{n}MSE_k\n",
    "  $$\n",
    "\n",
    "- For Classification, Compute \\* Where $Err_k = \\sum_{i\\in C_k}I(y_i\\neq \\hat{y_i})/n_k$\n",
    "  $$\n",
    "  CV_{(K)}=\\sum_{k=1}^K\\frac{n_k}{n}Err_k\n",
    "  $$\n",
    "- basically the weighted sum of $MSE_k$ by number of observations in $k$\n",
    "- Since each training set is only $(K-1)/K$, the **estimates of prediction error will typically be biased upward**.\n",
    "- $K=5$ or $10$ provides a good compromise for bias-variance tradeoff.\n",
    "\n",
    "![image-3.png](./img/31.png)\n",
    "\n",
    "### Example using Cross Validation\n",
    "\n",
    "![image-3.png](./img/32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common mistakes in Cross Validation\n",
    "\n",
    "![image-3.png](./img/33.png)\n",
    "\n",
    "#### **Feature selection before Cross validation**\n",
    "\n",
    "- Performing feature selection before cross-validation can lead to data leakage and overfitting, as you're peeking at the entire dataset to select features based on their correlation with the target variable.\n",
    "- This can result in **overly optimistic performance estimates**.\n",
    "\n",
    "#### Example\n",
    "\n",
    "- We randomly generate 50 predictors with different mean and variance normal distribution, and random label Y\n",
    "- We select 5 predictors having the largest correlation with the radom label\n",
    "- Then we put this 5 predictors into logistic regression to get CV Score\n",
    "- Theoretically, we will get CV score around 50% since the independent simulation, but result gives us much higher score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.55 0.7  0.45 0.75 0.75]\n",
      "Mean CV score: 0.64\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(5)\n",
    "\n",
    "# Generate X with 50 random arrays following normal distribution with different mean and variance\n",
    "X = np.random.normal(loc=np.random.uniform(-10, 10, size=50), scale=np.random.uniform(1, 5, size=50), size=(100, 50))\n",
    "\n",
    "# Generate y as random labels from uniform distribution\n",
    "y = np.random.randint(2, size=100)\n",
    "\n",
    "# Step 2: Select the 5 features most correlated with the outcome\n",
    "## This step is wrong since model have seen \n",
    "## all labels to get most correlated predictors\n",
    "correlations = np.array([pearsonr(X[:, i], y)[0] for i in range(X.shape[1])])\n",
    "top_five_indices = np.argsort(np.abs(correlations))[-5:]\n",
    "\n",
    "# Step 3: Perform cross-validation using only the selected features\n",
    "selected_X = X[:, top_five_indices]\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Use cross-validation to estimate the test error\n",
    "cv_scores = cross_val_score(model, selected_X, y, cv=5)\n",
    "\n",
    "# The mean cross-validated score can be misleadingly high due to feature selection bias\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(f'Mean CV score: {mean_cv_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap\n",
    "\n",
    "- The `bootstrap` is a flexible and powerful statistical tool that can be used to **quantify the uncertainty associated with a given estimator or statistical learning method**.\n",
    "  - For example, it provides the estimate of standard error of a coefficient, or a confidence interval for that coefficient.\n",
    "\n",
    "### Example for Bootstrap\n",
    "\n",
    "- We want to invest our money in two financial assets that yield returns of X and Y\n",
    "- We will invest a fraction $\\alpha$ of our money in $X$, and $1-\\alpha$ in Y\n",
    "- We wish to choose $\\alpha$ to minimize the total risk, or variance ($Var(\\alpha X+(1-\\alpha)Y)$), of our investment.\n",
    "  $$\n",
    "  Var(\\alpha X+(1-\\alpha)Y) = \\alpha ^2Var(X) + (1-\\alpha)^2 Var(Y) + 2\\alpha(1-\\alpha) CoV(XY)\n",
    "  $$\n",
    "  $$\n",
    "  \\frac{\\partial f}{\\partial \\alpha} = 2\\alpha Var(x) - 2(1-\\alpha) Var(Y) + 2(1-\\alpha) CoV(XY) - 2\\alpha(XY)\n",
    "  $$\n",
    "- Let $\\frac{\\partial f}{\\partial \\alpha}  = 0$, we get\n",
    "  $$\n",
    "  \\alpha = \\frac{Var(Y)-Cov(XY)}{Var(X)+Var(Y)-2CoV(XY)}\n",
    "  $$\n",
    "- We can estimate $Var(X), Var(Y)$ and $CoV(XY)$ using sampling / simulation\n",
    "\n",
    "![image-3.png](./img/34.png)\n",
    "\n",
    "![image-3.png](./img/35.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we need Bootstrap\n",
    "\n",
    "1. In real word analysis, we cannot generate new samples from the original population.\n",
    "\n",
    "   - Bootstrap allows us to mimic the process of obtaining new data sets. So we can estimate the variability of our estimate without generating additional samples\n",
    "   - Each bootstrap datasets is created by **sampling with replacement**, and is the **same size** as the original dataset.\n",
    "   - There is about a **two-thirds overlap** in a bootstrap sample with the original data\n",
    "\n",
    "   ![image-3.png](./img/36.png)\n",
    "\n",
    "2. Bootstrap **requires data being independent to each other**, some data like time series are not applicable for Bootstrap\n",
    "\n",
    "   - For time series, we can divide data by blocks, then use block to do bootstrap\n",
    "\n",
    "     ![image-3.png](./img/37.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
