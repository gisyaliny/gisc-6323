{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Logistic Regression\n",
                "\n",
                "## 1. Introduction to Classification\n",
                "In Linear Regression, we predict a continuous quantitative value (e.g., house price). However, in many real-world problems, we want to predict a **category** or **class**.\n",
                "\n",
                "Examples:\n",
                "*   **Email**: Is this email Spam or Not Spam?\n",
                "*   **Medical**: Does this patient have Heart Disease or Not?\n",
                "*   **Finance**: Will this customer Default on their loan or Not?\n",
                "\n",
                "These are **Classification** problems. The response variable $Y$ is qualitative (e.g., $Y \\in \\{0, 1\\}$).\n",
                "\n",
                "## 2. Why not Linear Regression?\n",
                "You might be tempted to use Linear Regression for a binary outcome (0 or 1). However, this has major issues:\n",
                "1.  **Unbounded Output**: Linear regression can predict values like -0.5 or 1.2, which don't make sense as probabilities.\n",
                "2.  **Violates Assumptions**: The errors are not normally distributed.\n",
                "\n",
                "Instead, we model the **Probability** that $Y$ belongs to a particular category."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Logistic Function\n",
                "To ensure our prediction falls between 0 and 1, we use the **Logistic Function** (Sigmoid function).\n",
                "\n",
                "$$ p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} $$\n",
                "\n",
                "*   If $\\beta_0 + \\beta_1 X$ is very large positive, $p(X) \\approx 1$.\n",
                "*   If $\\beta_0 + \\beta_1 X$ is very large negative, $p(X) \\approx 0$.\n",
                "\n",
                "This creates an **S-shaped curve** rather than a straight line."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Log-Odds (Logit)\n",
                "By rearranging the equation, we get linear relationship with the **log-odds**:\n",
                "\n",
                "$$ \\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1 X $$\n",
                "\n",
                "*   The quantity $\\frac{p(X)}{1-p(X)}$ is called the **Odds** (e.g., 4:1 odds means 80% probability).\n",
                "*   Increasing $X$ by one unit changes the **log-odds** by $\\beta_1$."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Estimating Coefficients (Maximum Likelihood)\n",
                "Unlike Linear Regression which uses Least Squares (minimizing error), Logistic Regression uses **Maximum Likelihood Estimation (MLE)**.\n",
                "\n",
                "**Intuition**: We search for $\\beta_0$ and $\\beta_1$ such that the predicted probabilities correspond as closely as possible to the observed individuals. \n",
                "*   If a person Has Disease ($Y=1$), we want their $p(X)$ to be close to 1.\n",
                "*   If a person No Disease ($Y=0$), we want their $p(X)$ to be close to 0."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Implementation Example\n",
                "We will use `sklearn` to implement Logistic Regression on a synthetic dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "# 1. Generate synthetic classification data\n",
                "# 1000 samples, 1 feature for easy visualization\n",
                "X, y = make_classification(n_samples=1000, n_features=1, n_informative=1, \n",
                "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
                "\n",
                "# 2. Split into Training and Testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "\n",
                "# 3. Fit the Logistic Regression Model\n",
                "log_reg = LogisticRegression()\n",
                "log_reg.fit(X_train, y_train)\n",
                "\n",
                "# 4. Making Predictions\n",
                "# predict_proba gives the probability (e.g., 0.85)\n",
                "y_prob = log_reg.predict_proba(X_test)[:, 1]\n",
                "# predict gives the class label (e.g., 1)\n",
                "y_pred = log_reg.predict(X_test)\n",
                "\n",
                "# 5. Evaluation\n",
                "print(\"Confusion Matrix:\")\n",
                "print(confusion_matrix(y_test, y_pred))\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualizing the S-Curve\n",
                "Below, we visualize how the logistic regression fits the data points (red) with a smooth sigmoid curve (blue). The decision boundary is at Probability = 0.5."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 6))\n",
                "\n",
                "# Scatter plot of actual test data (0 or 1)\n",
                "plt.scatter(X_test, y_test, color='red', alpha=0.3, label='Test Data Points')\n",
                "\n",
                "# Generate a range of X values to plot the smooth curve\n",
                "X_range = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)\n",
                "y_range_prob = log_reg.predict_proba(X_range)[:, 1]\n",
                "\n",
                "# Plot the Logistic Function\n",
                "plt.plot(X_range, y_range_prob, color='blue', linewidth=3, label='Logistic Sigmoid Curve')\n",
                "\n",
                "# Decision Boundary (0.5 probability)\n",
                "plt.axhline(0.5, color='gray', linestyle='--', label='Decision Boundary (P=0.5)')\n",
                "\n",
                "plt.xlabel('Feature Value')\n",
                "plt.ylabel('Probability of Class 1')\n",
                "plt.legend()\n",
                "plt.title('Logistic Regression Fit')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Quiz\n",
                "\n",
                "Test your understanding of Logistic Regression.\n",
                "\n",
                "**Q1. What is the range of output for the Logistic Function?**\n",
                "A) $(-\\infty, +\\infty)$\n",
                "B) $[0, 1]$\n",
                "C) $[-1, 1]$\n",
                "\n",
                "**Q2. If the coefficient $\\beta_1$ is positive, what does it imply?**\n",
                "A) Increasing X increases the probability of $Y=1$.\n",
                "B) Increasing X decreases the probability of $Y=1$.\n",
                "C) X has no effect on Y.\n",
                "\n",
                "**Q3. Which method is used to estimate parameters in Logistic Regression?**\n",
                "A) Least Squares\n",
                "B) Maximum Likelihood Estimation (MLE)\n",
                "C) Gini Index\n",
                "\n",
                "---\n",
                "### Sample Answers\n",
                "**Q1:** B) $[0, 1]$. Probabilities must always be between 0 and 1.\n",
                "**Q2:** A). A positive coefficient means the log-odds (and thus probability) increase as X increases.\n",
                "**Q3:** B). MLE is used to find the parameters that maximize the probability of the observed data."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}