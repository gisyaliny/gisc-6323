{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tree-Based Methods\n",
                "\n",
                "Tree-based methods involve **stratifying** or **segmenting** the predictor space into a number of simple regions. They are popular because they are easy to interpret and mimic human decision-making.\n",
                "\n",
                "## 1. Decision Trees\n",
                "\n",
                "### Ideally Simple\n",
                "Think of a game of \"20 Questions\". To identify an object, you ask yes/no questions that best split the possibilities. A Decision Tree does exactly this.\n",
                "\n",
                "### Terminology\n",
                "*   **Terminal Nodes (Leaves)**: The final regions/buckets where we assign a prediction.\n",
                "*   **Internal Nodes**: The points where the predictor space is split (e.g., $X_1 < 5$).\n",
                "*   **Branches**: The segments connecting nodes.\n",
                "\n",
                "### How to build a tree?\n",
                "We use **Recursive Binary Splitting**, a top-down, greedy approach.\n",
                "1.  **Top-down**: Start at the top of the tree with all observations.\n",
                "2.  **Greedy**: At each step, choose the **best split** that is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n",
                "\n",
                "### Classification Trees\n",
                "For classification, we cannot use Mean Squared Error (RSS). Instead, we aim for **Node Purity** (we want leaves to contain mostly one class). Measures include:\n",
                "*   **Gini Index**: A measure of total variance across the $K$ classes.\n",
                "*   **Entropy**: Another measure of impurity.\n",
                "\n",
                "Small Gini/Entropy indicates a pure node.\n",
                "\n",
                "### Pros & Cons\n",
                "*   **Pros**: Highly interpretable, can handle qualitative predictors without dummy variables.\n",
                "*   **Cons**: **High Variance**. A small change in data can cause a completely different tree."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# 1. Generate synthetic data\n",
                "X, y = make_classification(n_samples=500, n_features=2, n_informative=2, \n",
                "                           n_redundant=0, random_state=42, n_clusters_per_class=1)\n",
                "\n",
                "# 2. Split Data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "\n",
                "# 3. Fit a Single Decision Tree\n",
                "# max_depth limit helps prevent overfitting\n",
                "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
                "tree_clf.fit(X_train, y_train)\n",
                "\n",
                "# 4. Predict\n",
                "y_pred = tree_clf.predict(X_test)\n",
                "print(f\"Single Tree Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
                "\n",
                "# 5. Visualize the Tree\n",
                "plt.figure(figsize=(10,6))\n",
                "plot_tree(tree_clf, filled=True, feature_names=['Feature 1', 'Feature 2'], \n",
                "          class_names=['Class 0', 'Class 1'])\n",
                "plt.title(\"Decision Tree Visualization\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Ensemble Methods: Bagging & Random Forests\n",
                "\n",
                "To fix the **High Variance** problem of single trees, we use **Ensemble Methods** (combining many trees).\n",
                "\n",
                "### Bagging (Bootstrap Aggregation)\n",
                "Imagine asking 100 people to guess the number of jellybeans in a jar. The average of their guesses is usually better than any single guess. \n",
                "1.  Take $B$ bootstrap samples from training data.\n",
                "2.  Train a deep tree on each.\n",
                "3.  **Average** the predictions (regression) or take **Majority Vote** (classification).\n",
                "\n",
                "### Random Forests\n",
                "Bagging has a flaw: if there is one very strong predictor, all trees will use it as the top split, making the trees highly **correlated**. Averaging correlated trees doesn't reduce variance much.\n",
                "\n",
                "**Random Forest Solution**: At each split, we are ONLY allowed to consider a **random subset of predictors** ($m \\approx \\sqrt{p}$).\n",
                "*   This forces trees to specific details and generally **decorrelates** them.\n",
                "*   Result: Much more robust performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "# Fit Random Forest\n",
                "# n_estimators = number of trees\n",
                "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "rf_clf.fit(X_train, y_train)\n",
                "rf_pred = rf_clf.predict(X_test)\n",
                "\n",
                "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Boosting\n",
                "\n",
                "Boosting is a different approach. Instead of building independent trees in parallel (like Bagging/RF), we build trees **sequentially**.\n",
                "\n",
                "1.  Train a small tree.\n",
                "2.  Look at where it made mistakes (residuals).\n",
                "3.  Train the next tree specifically to fix those mistakes.\n",
                "4.  Add this new tree to the model (weighted by a learning rate).\n",
                "\n",
                "Boosting usually has lower Bias than Bagging, but can overfit if not tuned carefully."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "\n",
                "# Fit Gradient Boosting\n",
                "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
                "gb_clf.fit(X_train, y_train)\n",
                "gb_pred = gb_clf.predict(X_test)\n",
                "\n",
                "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, gb_pred):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Quiz\n",
                "\n",
                "**Q1. Why do we use Random Forests instead of just Bagging?**\n",
                "A) To increase the bias of the model.\n",
                "B) To decorrelate the trees and further reduce variance.\n",
                "C) To make the model faster to train.\n",
                "\n",
                "**Q2. Which method builds trees sequentially?**\n",
                "A) Decision Trees\n",
                "B) Random Forests\n",
                "C) Boosting\n",
                "\n",
                "**Q3. In a classification tree, what does a Gini Index of 0 mean?**\n",
                "A) Maximum impurity (node has equal mix of classes).\n",
                "B) The node is pure (all observations belong to one class).\n",
                "C) The tree has no branches.\n",
                "\n",
                "---\n",
                "### Sample Answers\n",
                "**Q1:** B). By restricting features at each split, RF ensures trees are diverse, making the average more stable.\n",
                "**Q2:** C). Boosting learns essentially from the errors of previous trees.\n",
                "**Q3:** B). Gini Index measures impurity. 0 means perfect purity."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}