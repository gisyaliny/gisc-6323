{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction Techniques\n",
    "\n",
    "Dimension reduction techniques aim to reduce the number of predictors by **transforming them into a lower-dimensional** space while **preserving most of the variability** in the data.\n",
    "\n",
    "- Dimension reduction techniques are effective for dealing with `multicollinearity` and high-dimensional data\n",
    "- The goal is constructing $M$ new predictors, which are linear combination of $p$ original predictors, and use the new predictors to make predictions.\n",
    "\n",
    "  - Let $Z_1,Z_2,...,Z_M$ represent the new predictors with $M<P$\n",
    "  - $\\phi_{m1},...,\\phi_{mj}$ are some constants\n",
    "    $$\n",
    "    Z_m = \\sum_{j=1}^{p}\\phi_{mj}X_j\n",
    "    $$\n",
    "  - Then we fit \\* $i=1,...,n$\n",
    "    $$\n",
    "    y_i = \\theta_0+\\sum_{m=1}^{M}\\theta_mZ_{im} + e_i\n",
    "    $$\n",
    "    ![image-2.png](./img/48.png)\n",
    "\n",
    "- But may result in **less interpretable** models compared to subset selection and shrinkage methods.\n",
    "\n",
    "### Principal Component Analysis\n",
    "\n",
    "- PCA transforms the original predictors into a new set of uncorrelated variables called principal components.\n",
    "- These components are ordered by the amount of variance they explain, allowing for dimensionality reduction by retaining only the most important components.\n",
    "\n",
    "![image-2.png](./img/49.png)\n",
    "\n",
    "- The first component summarize the most information from these two features\n",
    "\n",
    "![image-2.png](./img/50.png)\n",
    "\n",
    "- Not many information left on the 2nd component\n",
    "\n",
    "![image-2.png](./img/51.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Least Squares (PLS)\n",
    "\n",
    "- PCR assumes the principle components from predictors might have the best performance on dependent variable prediction (**unsupervised way**)\n",
    "\n",
    "  - This assumption might wrong\n",
    "  - We do not use the information from dependent variable to supervise the identification of the principal components.\n",
    "\n",
    "- PLS is also a dimension reduction technique\n",
    "  - But unlike PCR, PLS identify new features that not only approximate the old features well, **but also that are related to the target variable**\n",
    "  - Steps:\n",
    "    1. Standardize $p$ predictors\n",
    "    2. Construct simple Bivariate linear regression to get $\\theta_1,...,\\theta_p$\n",
    "    3. Compute the first direction $Z_1=\\sum_{j=1}^p\\theta_1jX_j$\n",
    "       1. PLS places the highest weight on the variables that most strongly related to the response.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
