{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to Matrix / Vectors\n",
    "\n",
    "Documents have different length, and consist of sequences of words. How do we create features X to characterize a document?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words\n",
    "\n",
    "- From a dictionary, identify the 10K most frequently occurring words.\n",
    "- Create a binary vector of length $p=10K$ for each document, and score a 1 in every position that the corresponding word occurred.\n",
    "  - With $n$ documents, we now have a $n*p$ **sparse** feature matrix X.\n",
    "\n",
    "#### Drawbacks\n",
    "\n",
    "- `Bag-of-Words (BoW)` **does not consider the sequence of words** in the text. It consider words independently, we call it `unigrams`. We can instead use `bigrams` (occurrences of adjacent word pairs), and in general `n-grams`.\n",
    "\n",
    "![image.png](./img/01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "N-grams are contiguous sequences of n items (words, characters, or tokens) extracted from a text corpus. In the context of natural language processing (NLP), N-grams are commonly used to capture patterns and dependencies between words in a sequence of text.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- **Capturing Context**: N-grams preserve some level of word order and context, allowing models to capture dependencies between adjacent words.\n",
    "- **Flexibility**: N-grams can be adjusted to different granularities (`unigrams`, `bigrams`, `trigrams`, etc.), providing flexibility in capturing different levels of context.\n",
    "\n",
    "#### Drawbacks\n",
    "\n",
    "- **Data Sparsity:** As the length of the n-grams increases, the number of unique combinations grows exponentially, leading to sparsity issues, especially with smaller datasets.\n",
    "- **Lack of Generalization**: N-grams may **overfit** to specific patterns present in the training data, making them less generalizable to unseen data.\n",
    "\n",
    "#### Mitigation Strategies\n",
    "\n",
    "- **Pruning**: Limit the vocabulary size or discard low-frequency n-grams to reduce computational complexity.\n",
    "- **Smoothing**: Address data sparsity issues by smoothing probabilities of unseen n-grams.\n",
    "\n",
    "![image.png](./img/02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- [Reference](https://www.kdnuggets.com/2022/09/convert-text-documents-tfidf-matrix-tfidfvectorizer.html)\n",
    "\n",
    "  $$\n",
    "  TF-IDF(t,d,D) = TF(t,d) * IDF(t,D)\n",
    "  $$\n",
    "\n",
    "### Term Frequency (TF)\n",
    "\n",
    "Measures how frequently a term (word) occurs in a document.\n",
    "\n",
    "- It is calculated as the ratio of the number of times a term appears in a document to the total number of terms in the document.\n",
    "  $$\n",
    "  TF(t,d) = \\frac{Number\\: of\\: occurrences\\: of\\: term\\: t\\: in\\: document\\: d}{Total\\: number\\: of\\: terms\\: in\\: document\\: d}\n",
    "  $$\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "Measures **how important a term is** across the entire collection of documents.\n",
    "\n",
    "- It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term, with a smoothing term to avoid division by zero.\n",
    "\n",
    "  $$\n",
    "  IDF(t,D) = log(\\frac{Number\\: of \\: documents}{Number\\: of\\: documents\\: containing\\: term\\: t})\n",
    "  $$\n",
    "\n",
    "- **Text A**: Jupiter is the largest planet\n",
    "- **Text B**: Mars is the fourth planet from the sun\n",
    "\n",
    "![image.png](./img/03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks\n",
    "\n",
    "- `BoW` and `TF-IDF` represent words as sparse vectors of counts or weighted frequencies, respectively, without **capturing semantic relationships between words**.\n",
    "- They may **struggle to handle out-of-vocabulary words** and fail to capture subtle semantic similarities between words.\n",
    "\n",
    "### Example\n",
    "\n",
    "- In this tutorial, we are going to use `TfidfVectorizer` from `scikit-learn` to convert the text and view the `TF-IDF` matrix.\n",
    "- In the code below, we have a small corpus of 4 documents.\n",
    "  - First, we will create a vectorizer object using `TfidfVectorizer()` and fit and transform the text data into vectors.\n",
    "  - After that, we will use vectorizers to extract the names of the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "          'KDnuggets Collection of data science Projects',\n",
    "          '3 Free Statistics Courses for data science',\n",
    "          'Parallel Processing Large File in Python',\n",
    "          '15 Python Coding Interview Questions You Must Know For data science',\n",
    " ]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# TD-IDF Matrix\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# extracting feature names\n",
    "tfidf_tokens = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use TF-IDF tokens and vectors to create a pandas dataframe.\n",
    "\n",
    "1. Convert the vectors to arrays and add it to the data argument.\n",
    "2. Four indexes are created manually.\n",
    "3. `tfidf_tokens` names are added to columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>15</th>\n",
       "      <th>coding</th>\n",
       "      <th>collection</th>\n",
       "      <th>courses</th>\n",
       "      <th>data</th>\n",
       "      <th>file</th>\n",
       "      <th>for</th>\n",
       "      <th>free</th>\n",
       "      <th>in</th>\n",
       "      <th>interview</th>\n",
       "      <th>...</th>\n",
       "      <th>must</th>\n",
       "      <th>of</th>\n",
       "      <th>parallel</th>\n",
       "      <th>processing</th>\n",
       "      <th>projects</th>\n",
       "      <th>python</th>\n",
       "      <th>questions</th>\n",
       "      <th>science</th>\n",
       "      <th>statistics</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474771</td>\n",
       "      <td>0.303040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374315</td>\n",
       "      <td>0.474771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303040</td>\n",
       "      <td>0.474771</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421765</td>\n",
       "      <td>0.421765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.332524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.332264</td>\n",
       "      <td>0.332264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.332264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261961</td>\n",
       "      <td>0.332264</td>\n",
       "      <td>0.212080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.332264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            15    coding  collection   courses      data      file       for  \\\n",
       "Doc1  0.000000  0.000000    0.455732  0.000000  0.290888  0.000000  0.000000   \n",
       "Doc2  0.000000  0.000000    0.000000  0.474771  0.303040  0.000000  0.374315   \n",
       "Doc3  0.000000  0.000000    0.000000  0.000000  0.000000  0.421765  0.000000   \n",
       "Doc4  0.332264  0.332264    0.000000  0.000000  0.212080  0.000000  0.261961   \n",
       "\n",
       "          free        in  interview  ...      must        of  parallel  \\\n",
       "Doc1  0.000000  0.000000   0.000000  ...  0.000000  0.455732  0.000000   \n",
       "Doc2  0.474771  0.000000   0.000000  ...  0.000000  0.000000  0.000000   \n",
       "Doc3  0.000000  0.421765   0.000000  ...  0.000000  0.000000  0.421765   \n",
       "Doc4  0.000000  0.000000   0.332264  ...  0.332264  0.000000  0.000000   \n",
       "\n",
       "      processing  projects    python  questions   science  statistics  \\\n",
       "Doc1    0.000000  0.455732  0.000000   0.000000  0.290888    0.000000   \n",
       "Doc2    0.000000  0.000000  0.000000   0.000000  0.303040    0.474771   \n",
       "Doc3    0.421765  0.000000  0.332524   0.000000  0.000000    0.000000   \n",
       "Doc4    0.000000  0.000000  0.261961   0.332264  0.212080    0.000000   \n",
       "\n",
       "           you  \n",
       "Doc1  0.000000  \n",
       "Doc2  0.000000  \n",
       "Doc3  0.000000  \n",
       "Doc4  0.332264  \n",
       "\n",
       "[4 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(\n",
    "    data=X.toarray(), \n",
    "    index=[\"Doc1\", \"Doc2\", \"Doc3\", \"Doc4\"], \n",
    "    columns=tfidf_tokens\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings (Word2Vec, FastText)\n",
    "\n",
    "- [Reference](https://kavita-ganesan.com/fasttext-vs-word2vec/)\n",
    "\n",
    "  Word Embeddings are dense vector representations of words in a continuous vector space, where **words with similar meanings are represented by vectors that are close together**.\n",
    "\n",
    "![image.png](./img/04.png)\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "- `Word2Vec`, introduced by Mikolov et al. in 2013, aims to learn word embeddings by predicting the context of words in a large corpus. It uses shallow neural networks to learn distributed representations of words based on their co-occurrence patterns.\n",
    "- `Word2Vec` uses a `softmax` function and `negative sampling` or `hierarchical softmax` to train the neural network efficiently.\n",
    "- `Word2Vec` consists of two primary architectures: `Continuous Bag of Words (CBOW)` and `Skip-gram`.\n",
    "  - In `CBOW`, the model predicts the target word based on its context words\n",
    "  - In `Skip-gram`, the model predicts context words given the target word.\n",
    "\n",
    "![image.png](./img/05.png)\n",
    "\n",
    "### FastText\n",
    "\n",
    "- `FastText`, also developed by Mikolov et al., extends `Word2Vec` by **considering subword information**. It learns embeddings **not only for complete words but also for character n-grams**, allowing it to capture morphological information and handle out-of-vocabulary words better.\n",
    "- `FastText` extends `Word2Vec` by representing words as the sum of the embeddings of their `character n-grams`.\n",
    "  - A `character n-gram` is a set of co-occurring characters within a given window. It’s very similar to `word n-grams`, only that the window size is at the character level.\n",
    "  - It learns embeddings for **both words and character n-grams separately and combines them to generate the final word embeddings**.\n",
    "\n",
    "![image.png](./img/06.png)\n",
    "\n",
    "### Drawbacks\n",
    "\n",
    "- Word embeddings **require a large corpus of text data** for training and may not perform well with small datasets.\n",
    "- They also rely heavily on the quality of the training data and may encode biases present in the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe (Global Vectors for Word Representation)\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) is a word embedding model introduced by Jeffrey Pennington, Richard Socher, and Christopher D. Manning in 2014. It is an **unsupervised learning** algorithm used for obtaining **vector representations** for words.\n",
    "\n",
    "### Design Purpose:\n",
    "\n",
    "- GloVe constructs a co-occurrence matrix where each entry represents the number of times a word co-occurs with another word in the global corpus.\n",
    "- It then learns word vectors by minimizing a weighted least squares regression objective function that captures the relationship between word vectors and their co-occurrence probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage\n",
    "\n",
    "- **Global Co-occurrence Information**: GloVe leverages global `word co-occurrence statistics`, capturing richer semantic relationships between words beyond local contexts. It considers the `entire corpus` when learning word embeddings, which can lead to more informative embeddings.\n",
    "- **Robustness to Out-of-Vocabulary Words**: GloVe can handle out-of-vocabulary words better than Word2Vec and FastText by leveraging global statistics. It can capture semantic similarities between words even if they do not appear frequently in local contexts.\n",
    "- **Interpretability and Generalization**: GloVe embeddings often exhibit good interpretability and generalization capabilities due to their reliance on global co-occurrence statistics. They tend to capture broader semantic relationships and may generalize well to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks\n",
    "\n",
    "- **Memory Intensive**: GloVe requires storing a large co-occurrence matrix in memory, which can be memory-intensive, especially for very large corpora.\n",
    "- **Training Complexity**: Although GloVe's objective function is relatively simple compared to neural network-based methods like Word2Vec, training GloVe still involves optimizing a large number of parameters.\n",
    "- **Fixed Context Window**: GloVe typically operates using a fixed context window for calculating co-occurrence statistics. While this window size can be adjusted, it still represents a limitation in capturing contextual information compared to methods that dynamically adjust the context window based on word frequency or document structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of co-occurrence matrix\n",
    "\n",
    "Consider the following small corpus consisting of three sentences:\n",
    "\n",
    "1. \"I like natural language processing.\"\n",
    "2. \"I enjoy deep learning.\"\n",
    "3. \"I prefer natural language models.\"\n",
    "\n",
    "GloVe calculates the co-occurrence matrix by counting how often each word appears in the context of other words within a certain window size. Let's assume a window size of 1 (i.e., considering adjacent words as context).\n",
    "\n",
    "```\n",
    "            | I | like | natural | language | processing | enjoy | deep | prefer | models |\n",
    "-------------------------------------------------------------------------------------------\n",
    "I           | 0 | 1    | 2       | 0        | 0          | 1     | 0    | 1      | 0      |\n",
    "like        | 1 | 0    | 1       | 0        | 0          | 0     | 0    | 0      | 0      |\n",
    "natural     | 2 | 1    | 0       | 1        | 1          | 0     | 0    | 1      | 1      |\n",
    "language    | 0 | 0    | 1       | 0        | 1          | 0     | 0    | 0      | 1      |\n",
    "processing  | 0 | 0    | 1       | 1        | 0          | 0     | 0    | 0      | 0      |\n",
    "enjoy       | 1 | 0    | 0       | 0        | 0          | 0     | 1    | 0      | 0      |\n",
    "deep        | 0 | 0    | 0       | 0        | 0          | 1     | 0    | 0      | 0      |\n",
    "prefer      | 1 | 0    | 1       | 0        | 0          | 0     | 0    | 0      | 1      |\n",
    "models      | 0 | 0    | 1       | 1        | 0          | 0     | 0    | 1      | 0      |\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['like', 'processing.', 'deep', 'models.', 'language', 'learning.', 'enjoy', 'I', 'natural', 'prefer']\n",
      "Co-occurrence matrix:\n",
      "[[0 0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0 0 2 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 1]\n",
      " [1 0 0 0 2 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_co_occurrence_matrix(corpus, window_size=1):\n",
    "    vocabulary = set()\n",
    "    co_occurrence_matrix = {}\n",
    "\n",
    "    # Tokenize the corpus\n",
    "    tokenized_corpus = [sentence.split() for sentence in corpus]\n",
    "\n",
    "    # Build vocabulary and co-occurrence matrix\n",
    "    for tokens in tokenized_corpus:\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token not in vocabulary:\n",
    "                vocabulary.add(token)\n",
    "                co_occurrence_matrix[token] = {}\n",
    "            \n",
    "            for j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    context_token = tokens[j]\n",
    "                    if context_token not in co_occurrence_matrix[token]:\n",
    "                        co_occurrence_matrix[token][context_token] = 0\n",
    "                    co_occurrence_matrix[token][context_token] += 1\n",
    "\n",
    "    # Convert co-occurrence matrix to numpy array\n",
    "    vocab_list = list(vocabulary)\n",
    "    matrix_size = len(vocabulary)\n",
    "    co_occurrence_array = np.zeros((matrix_size, matrix_size), dtype=np.int32)\n",
    "    \n",
    "    for i, token1 in enumerate(vocab_list):\n",
    "        for j, token2 in enumerate(vocab_list):\n",
    "            if token2 in co_occurrence_matrix[token1]:\n",
    "                co_occurrence_array[i, j] = co_occurrence_matrix[token1][token2]\n",
    "\n",
    "    return co_occurrence_array, vocab_list\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"I like natural language processing.\",\n",
    "    \"I enjoy deep learning.\",\n",
    "    \"I prefer natural language models.\"\n",
    "]\n",
    "\n",
    "co_occurrence_matrix, vocabulary = create_co_occurrence_matrix(corpus, window_size=1)\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Co-occurrence matrix:\")\n",
    "print(co_occurrence_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualized word embeddings (ELMo and BERT)\n",
    "\n",
    "### ELMo (Embeddings from Language Models)\n",
    "\n",
    "- [Reference](https://ireneli.eu/2018/12/17/elmo-in-practice/)\n",
    "- **Design Purpose:** ELMo was introduced by Matthew Peters and colleagues in 2018. It leverages deep contextualized word representations learned from `bidirectional language models` trained on large text corpora.\n",
    "  - `ELMo` generates word embeddings by considering the entire input sentence and capturing the contextual meaning of words.\n",
    "- **Model Architecture**: `ELMo` uses a deep `bidirectional LSTM (Long Short-Term Memory)` language model to learn contextual representations of words.\n",
    "  - The model is trained to **predict the next word** in a sentence **given both the preceding and following words**, allowing it to capture complex syntactic and semantic dependencies.\n",
    "- **Benefits**: `ELMo` embeddings are context-sensitive and capture fine-grained syntactic and semantic information.\n",
    "  - They can handle polysemy, word sense disambiguation, and syntactic nuances effectively, making them suitable for a wide range of NLP tasks, including sentiment analysis, named entity recognition, and question answering.\n",
    "\n",
    "![image.png](./img/07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "- [Reference](https://www.geeksforgeeks.org/understanding-bert-nlp/)\n",
    "- **Design Purpose**: `BERT`, introduced by Jacob Devlin and colleagues at Google AI in 2018, is a `transformer-based model` designed to generate deep contextualized word representations.\n",
    "  - BERT is pretrained on large-scale text corpora using masked language modeling and **next sentence prediction tasks**, enabling it to capture bidirectional context information.\n",
    "- **Model Architecture**: BERT consists of a stack of **transformer encoder layers** that process input sequences bidirectionally.\n",
    "  - During `pretraining`, BERT masks some tokens in the input and predicts them based on the remaining context. This enables the model to learn contextualized embeddings that capture both local and global context information.\n",
    "- **Benefits**: BERT embeddings capture rich contextual information and can model complex linguistic phenomena, including word ambiguity, syntactic structure, and discourse coherence.\n",
    "  - BERT embeddings have achieved state-of-the-art performance on a wide range of NLP tasks, **including question answering, text classification, and natural language inference.**\n",
    "\n",
    "![image.png](./img/08.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling (LDA, LSA)\n",
    "\n",
    "Topic Modeling is a technique used in natural language processing (NLP) to discover the **latent thematic structure** present in a collection of documents.\n",
    "\n",
    "- It aims to **identify topics or themes** that pervade a corpus and the distribution of these topics across the documents.\n",
    "- Two popular methods for topic modeling are `Latent Dirichlet Allocation (LDA)` and `Latent Semantic Analysis (LSA)`.\n",
    "\n",
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- [Reference](https://www.analyticsvidhya.com/blog/2021/06/part-2-topic-modeling-and-latent-dirichlet-allocation-lda-using-gensim-and-sklearn/)\n",
    "- **Design Purpose**: `LDA`, introduced by Blei et al. in 2003, is a probabilistic generative model for topic modeling.\n",
    "  - It assumes that documents are represented as a mixture of topics, and **each topic is represented as a distribution over words**.\n",
    "  - LDA aims to infer the underlying topic structure by estimating the distribution of topics in each document and the distribution of words in each topic.\n",
    "- **Model Assumptions**: LDA makes two key assumptions:\n",
    "  1. Each document is a mixture of a small number of topics.\n",
    "  2. Each word in a document is generated from one of the topics in the document's topic mixture.\n",
    "- **Model Inference**: LDA uses Bayesian inference techniques, such as `variational inference` or `Gibbs sampling`, to estimate the posterior distribution of topics given the observed documents.\n",
    "  - It iteratively updates the topic assignments of words in each document to find the most likely topic structure.\n",
    "\n",
    "![image.png](./img/09.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
