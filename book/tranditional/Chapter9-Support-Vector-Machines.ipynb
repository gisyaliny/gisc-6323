{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Support Vector Machines (SVM)\n",
                "\n",
                "Support Vector Machines are among the best \"out-of-the-box\" classifiers available. They rely on elegant geometric concepts to find the optimal boundary between classes.\n",
                "\n",
                "## 1. The Hyperplane Concept\n",
                "*   In **2 Dimensions**, a hyperplane is a flat **Line**.\n",
                "*   In **3 Dimensions**, a hyperplane is a flat **Plane**.\n",
                "*   In **p Dimensions**, it is a $p-1$ dimensional subspace.\n",
                "\n",
                "We want to find a hyperplane that separates our two classes of data (e.g., Blue dots vs Red dots) perfectly.\n",
                "\n",
                "## 2. Maximal Margin Classifier\n",
                "Usually, there are infinite lines that can separate two perfectly separated clusters. Which one is best?\n",
                "\n",
                "**Intuition**: We want the \"widest street\" possible.\n",
                "*   We find a separator such that the distance to the nearest training data points is maximized. \n",
                "*   This distance is called the **Margin**.\n",
                "*   The classifier is the \"center line\" of this street.\n",
                "\n",
                "### Support Vectors\n",
                "Interestingly, the position of the decision boundary depends **only** on the few observations that are closest to the line.\n",
                "*   These closest points are called **Support Vectors**.\n",
                "*   Points far away from the boundary do not affect the model at all. This makes SVM distinct from Logistic Regression (where all points contribute).\n",
                "\n",
                "## 3. The Kernel Trick (Non-Linearity)\n",
                "What if data isn't separable by a straight line? (e.g., data looks like a circle inside a ring).\n",
                "\n",
                "**Solution**: Project the data into a higher dimension.\n",
                "*   Imagine 2D data on a sheet of paper. You can't draw a straight line to separate inner and outer rings.\n",
                "*   \"Lift\" the inner ring up into 3D space.\n",
                "*   Now you can slide a flat sheet (hyperplane) between them!\n",
                "*   This mathematical projection is handled efficiently using **Kernels**.\n",
                "\n",
                "Common Kernels:\n",
                "*   **Linear**: Standard straight line.\n",
                "*   **Polynomial**: Curved lines.\n",
                "*   **Radial Basis Function (RBF)**: Can create complex, island-like shapes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.datasets import make_moons\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# 1. Generate Non-Linear Data (Moons)\n",
                "X, y = make_moons(n_samples=400, noise=0.2, random_state=42)\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "\n",
                "# Visualize Data\n",
                "plt.figure(figsize=(8,6))\n",
                "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
                "plt.title('Data that is NOT linearly separable')\n",
                "plt.show()\n",
                "\n",
                "# Helper function to visualize boundaries\n",
                "def plot_decision_boundary(model, X, y, title=\"Boundary\"):\n",
                "    h = .02\n",
                "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
                "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
                "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
                "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "    Z = Z.reshape(xx.shape)\n",
                "    plt.figure(figsize=(8,6))\n",
                "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
                "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
                "    plt.title(title)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Comparing Kernels\n",
                "We will try a Linear Kernel (which should fail) and an RBF Kernel (which should succeed)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Linear Kernel\n",
                "svm_linear = SVC(kernel='linear', C=1.0)\n",
                "svm_linear.fit(X_train, y_train)\n",
                "acc_linear = accuracy_score(y_test, svm_linear.predict(X_test))\n",
                "plot_decision_boundary(svm_linear, X, y, f\"Linear Kernel (Acc: {acc_linear:.2f})\")\n",
                "\n",
                "# 3. RBF Kernel (Radial Basis Function)\n",
                "svm_rbf = SVC(kernel='rbf', gamma=2, C=1.0)\n",
                "svm_rbf.fit(X_train, y_train)\n",
                "acc_rbf = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
                "plot_decision_boundary(svm_rbf, X, y, f\"RBF Kernel (Acc: {acc_rbf:.2f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hyperparameters\n",
                "Two key parameters control SVM behavior:\n",
                "\n",
                "1.  **C (Cost)**: Controls how strict we are about the margin.\n",
                "    *   **Low C**: \"Soft margin\". Allows more errors/violations used to find a wider street. Generally generalizes better (Low Variance).\n",
                "    *   **High C**: \"Hard margin\". Strict. Tries to classify everything correctly. Can overfit (High Variance).\n",
                "\n",
                "2.  **Gamma** (for RBF): Defines how far the influence of a single training example reaches.\n",
                "    *   **Low Gamma**: Far reach. Smoother decision boundary.\n",
                "    *   **High Gamma**: Close reach. Boundary hugs data points tightly (can lead to \"islands\" around points).\n",
                "\n",
                "## 5. Quiz\n",
                "\n",
                "**Q1. Which Kernel is best suited for concentric circle data (one circle inside another)?**\n",
                "A) Linear Kernel\n",
                "B) RBF or Polynomial Kernel\n",
                "C) No SVM can handle this.\n",
                "\n",
                "**Q2. What are \"Support Vectors\"?**\n",
                "A) All data points in the training set.\n",
                "B) The data points closest to the decision boundary (margin).\n",
                "C) The misclassified points only.\n",
                "\n",
                "**Q3. If your SVM is overfitting (memorizing the noise), what should you try?**\n",
                "A) Increase C (make it stricter).\n",
                "B) Decrease C (allow wider margin/smoother boundary).\n",
                "C) Use a more complex Kernel.\n",
                "\n",
                "---\n",
                "### Sample Answers\n",
                "**Q1:** B). RBF/Polynomial project data to higher dims where circles become separable planes.\n",
                "**Q2:** B). They literally \"support\" or define the boundary.\n",
                "**Q3:** B). Decreasing C increases regularization (wider margin), reducing overfitting."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}