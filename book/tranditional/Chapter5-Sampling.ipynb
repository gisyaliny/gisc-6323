{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sampling\n",
                "\n",
                "- Sometimes we need to test the model performance, but we can not use the same data as training and testing. therefore we need sampling strategies.\n",
                "- **Training error** (blue line): Training error, also known as in-sample error, is the error that occurs when a model is applied to the data it was trained on.\n",
                "- **Testing error** (red line): Testing error, or out-of-sample error, is the error that occurs when the model is applied to a new dataset that it hasn't seen before, often a validation or test set.\n",
                "\n",
                "![image-3.png](./img/29.png)\n",
                "\n",
                "## Estimate Testing error\n",
                "\n",
                "- When we use training error to estimate testing error, we often get a **underestimated** result. Some methods make a **mathematical adjustment** to the training error rate in order to estimate the test error rate.\n",
                "  - $-2ln(L)$ use the same way as RSS, in linear regression, $-2log(L) = RSS/\\sigma^2$\n",
                "\n",
                "### AIC (Akaike Information Criterion)\n",
                "\n",
                "$$\n",
                "AIC=2k-2ln(L)\n",
                "$$\n",
                "\n",
                "- $k$ is the number of parameters in the model\n",
                "- $L$ is the maximum value of the likelihood function for the model\n",
                "  - Normally, we assume **errors are normally distributed** (offsets between expectation and observation)\n",
                "\n",
                "### BIC (Bayesian Information Criterion)\n",
                "\n",
                "$$\n",
                "BIC=ln(n)k-2ln(L)\n",
                "$$\n",
                "\n",
                "- $n$ is the number of observations\n",
                "- The other terms are as defined for AIC\n",
                "- when $n > 7$, $BIC$ put more penalty than $AIC$\n",
                "\n",
                "### Example\n",
                "\n",
                "- Want to compare linear vs higher-order polynomial terms in a linear regression\n",
                "\n",
                "![image-3.png](./img/30.png)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import statsmodels.api as sm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Best model by AIC is of degree 2 with AIC = 425.7373213865948\n",
                        "Best model by BIC is of degree 2 with BIC = 433.55283194455905\n"
                    ]
                }
            ],
            "source": [
                "# Generate some data\n",
                "np.random.seed(0)\n",
                "X = np.random.uniform(-10, 10, 100)\n",
                "Y = 3 - 2*X + X**2 + np.random.normal(0, 2, 100)\n",
                "\n",
                "# Fit models of varying complexity\n",
                "results = {}\n",
                "for degree in range(1, 6):\n",
                "    features = np.column_stack([X**i for i in range(1, degree + 1)])\n",
                "    features = sm.add_constant(features)\n",
                "    model = sm.OLS(Y, features).fit()\n",
                "    results[degree] = {'model': model, 'AIC': model.aic, 'BIC': model.bic}\n",
                "\n",
                "# Find the best model according to AIC and BIC\n",
                "best_by_aic = min(results, key=lambda k: results[k]['AIC'])\n",
                "best_by_bic = min(results, key=lambda k: results[k]['BIC'])\n",
                "\n",
                "print(f'Best model by AIC is of degree {best_by_aic} with AIC = {results[best_by_aic][\"AIC\"]}')\n",
                "print(f'Best model by BIC is of degree {best_by_bic} with BIC = {results[best_by_bic][\"BIC\"]}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cross Validation\n",
                "\n",
                "- In normal validation approach, **only a subset of the observation are used to fit the model**\n",
                "  - The validation error may tend to **overestimate the test error** for the model fit on the entire data set\n",
                "  - For some shrinkage method (e.g. Lasso and Ridge regression), it's hard to find the number of predictors $k$\n",
                "  - Cross validation provide a direct way to test error, instead of using the variance of error $\\sigma^2$\n",
                "\n",
                "### K-fold Cross Validation\n",
                "\n",
                "- Let the $K$ parts be $C_1,C_2,...,C_K$, where $C_k$ denotes the indices of the observations in part $k$. There're $n_k$ observations in part k, where $n_k=n/K$\n",
                "- Compute (regression)\n",
                "\n",
                "  $$\n",
                "  CV_{(K)}=\\sum_{k=1}^K\\frac{n_k}{n}MSE_k\n",
                "  $$\n",
                "\n",
                "- For Classification, Compute \\* Where $Err_k = \\sum_{i\\in C_k}I(y_i\\neq \\hat{y_i})/n_k$\n",
                "  $$\n",
                "  CV_{(K)}=\\sum_{k=1}^K\\frac{n_k}{n}Err_k\n",
                "  $$\n",
                "- basically the weighted sum of $MSE_k$ by number of observations in $k$\n",
                "- Since each training set is only $(K-1)/K$, the **estimates of prediction error will typically be biased upward**.\n",
                "- $K=5$ or $10$ provides a good compromise for bias-variance tradeoff.\n",
                "\n",
                "![image-3.png](./img/31.png)\n",
                "\n",
                "### Example using Cross Validation\n",
                "\n",
                "![image-3.png](./img/32.png)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Common mistakes in Cross Validation\n",
                "\n",
                "![image-3.png](./img/33.png)\n",
                "\n",
                "#### **Feature selection before Cross validation**\n",
                "\n",
                "- Performing feature selection before cross-validation can lead to data leakage and overfitting, as you're peeking at the entire dataset to select features based on their correlation with the target variable.\n",
                "- This can result in **overly optimistic performance estimates**.\n",
                "\n",
                "#### Example\n",
                "\n",
                "- We randomly generate 50 predictors with different mean and variance normal distribution, and random label Y\n",
                "- We select 5 predictors having the largest correlation with the radom label\n",
                "- Then we put this 5 predictors into logistic regression to get CV Score\n",
                "- Theoretically, we will get CV score around 50% since the independent simulation, but result gives us much higher score\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from scipy.stats import pearsonr"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cross-validation scores: [0.55 0.7  0.45 0.75 0.75]\n",
                        "Mean CV score: 0.64\n"
                    ]
                }
            ],
            "source": [
                "# Set random seed for reproducibility\n",
                "np.random.seed(5)\n",
                "\n",
                "# Generate X with 50 random arrays following normal distribution with different mean and variance\n",
                "X = np.random.normal(loc=np.random.uniform(-10, 10, size=50), scale=np.random.uniform(1, 5, size=50), size=(100, 50))\n",
                "\n",
                "# Generate y as random labels from uniform distribution\n",
                "y = np.random.randint(2, size=100)\n",
                "\n",
                "# Step 2: Select the 5 features most correlated with the outcome\n",
                "## This step is wrong since model have seen \n",
                "## all labels to get most correlated predictors\n",
                "correlations = np.array([pearsonr(X[:, i], y)[0] for i in range(X.shape[1])])\n",
                "top_five_indices = np.argsort(np.abs(correlations))[-5:]\n",
                "\n",
                "# Step 3: Perform cross-validation using only the selected features\n",
                "selected_X = X[:, top_five_indices]\n",
                "model = LogisticRegression()\n",
                "\n",
                "# Use cross-validation to estimate the test error\n",
                "cv_scores = cross_val_score(model, selected_X, y, cv=5)\n",
                "\n",
                "# The mean cross-validated score can be misleadingly high due to feature selection bias\n",
                "mean_cv_score = np.mean(cv_scores)\n",
                "print(\"Cross-validation scores:\", cv_scores)\n",
                "print(f'Mean CV score: {mean_cv_score}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bootstrap\n",
                "\n",
                "- The `bootstrap` is a flexible and powerful statistical tool that can be used to **quantify the uncertainty associated with a given estimator or statistical learning method**.\n",
                "- For example, it provides the estimate of standard error of a coefficient, or a confidence interval for that coefficient.\n",
                "\n",
                "### Example for Bootstrap\n",
                "\n",
                "- We want to invest our money in two financial assets that yield returns of X and Y\n",
                "- We will invest a fraction $\\alpha$ of our money in $X$, and $1-\\alpha$ in Y\n",
                "- We wish to choose $\\alpha$ to minimize the total risk, or variance ($Var(\\alpha X+(1-\\alpha)Y)$), of our investment.\n",
                "  $$\n",
                "  Var(\\alpha X+(1-\\alpha)Y) = \\alpha ^2Var(X) + (1-\\alpha)^2 Var(Y) + 2\\alpha(1-\\alpha) CoV(XY)\n",
                "  $$\n",
                "  $$\n",
                "  \\frac{\\partial f}{\\partial \\alpha} = 2\\alpha Var(x) - 2(1-\\alpha) Var(Y) + 2(1-\\alpha) CoV(XY) - 2\\alpha(XY)\n",
                "  $$\n",
                "- Let $\\frac{\\partial f}{\\partial \\alpha}  = 0$, we get\n",
                "  $$\n",
                "  \\alpha = \\frac{Var(Y)-Cov(XY)}{Var(X)+Var(Y)-2CoV(XY)}\n",
                "  $$\n",
                "- We can estimate $Var(X), Var(Y)$ and $CoV(XY)$ using sampling / simulation\n",
                "\n",
                "![image-3.png](./img/34.png)\n",
                "\n",
                "![image-3.png](./img/35.png)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Why we need Bootstrap\n",
                "\n",
                "1. In real word analysis, we cannot generate new samples from the original population.\n",
                "\n",
                "   - Bootstrap allows us to mimic the process of obtaining new data sets. So we can estimate the variability of our estimate without generating additional samples\n",
                "   - Each bootstrap datasets is created by **sampling with replacement**, and is the **same size** as the original dataset.\n",
                "   - There is about a **two-thirds overlap** in a bootstrap sample with the original data\n",
                "\n",
                "   ![image-3.png](./img/36.png)\n",
                "\n",
                "2. Bootstrap **requires data being independent to each other**, some data like time series are not applicable for Bootstrap\n",
                "\n",
                "   - For time series, we can divide data by blocks, then use block to do bootstrap\n",
                "\n",
                "     ![image-3.png](./img/37.png)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Bootstrap Implementation Example\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.utils import resample\n",
                "import numpy as np\n",
                "\n",
                "# Original dataset\n",
                "data = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
                "\n",
                "# prepare bootstrap sample\n",
                "boot = resample(data, replace=True, n_samples=len(data), random_state=1)\n",
                "print('Bootstrap Sample:', boot)\n",
                "\n",
                "# out of bag observations\n",
                "oob = [x for x in data if x not in boot]\n",
                "print('OOB Sample:', oob)\n",
                "\n",
                "# Bootstrap estimates of the mean\n",
                "means = []\n",
                "for i in range(1000):\n",
                "    boot = resample(data, replace=True, n_samples=len(data))\n",
                "    means.append(np.mean(boot))\n",
                "\n",
                "print(\"Original Mean:\", np.mean(data))\n",
                "print(\"Bootstrap Mean:\", np.mean(means))\n",
                "print(\"Bootstrap Standard Error:\", np.std(means))\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "mybase",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}