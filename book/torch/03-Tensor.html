
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3. Tensor &#8212; Machine Learning for Socio-Economic and Georeferenced Data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/torch/03-Tensor';</script>
    <link rel="canonical" href="https://gisyaliny.github.io/gisc-6323/book/torch/03-Tensor.html" />
    <link rel="icon" href="../../_static/fav.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="2. Keras installation" href="02-tensorflow-installation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Machine Learning for Socio-Economic and Georeferenced Data - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Machine Learning for Socio-Economic and Georeferenced Data - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction &amp; Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Intro/author.html">About the Author</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro/preface.html">Preface</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning with R torch</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-torch-installation.html">1. <code class="docutils literal notranslate"><span class="pre">torch</span></code> Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-tensorflow-installation.html">2. <code class="docutils literal notranslate"><span class="pre">Keras</span></code> installation</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3. Tensor</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gisyaliny/gisc-6323" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gisyaliny/gisc-6323/edit/main/book/torch/03-Tensor.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gisyaliny/gisc-6323/issues/new?title=Issue%20on%20page%20%2Fbook/torch/03-Tensor.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/book/torch/03-Tensor.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Tensor</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-in-a-tensor">3.1. What’s in a tensor?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-tensors">3.2. Creating tensors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors-from-values">3.2.1. Tensors from values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors-from-specifications">3.2.2. Tensors from specifications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors-from-datasets">3.2.3. Tensors from datasets</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operations-on-tensors">3.3. Operations on tensors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-operations">3.3.1. Summary operations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-parts-of-a-tensor">3.4. Accessing parts of a tensor</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-and-slicing">3.4.1. Indexing and Slicing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-query">3.4.2. Advanced query</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reshaping-tensors">3.5. Reshaping tensors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting">3.6. Broadcasting</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="tensor">
<h1><span class="section-number">3. </span>Tensor<a class="headerlink" href="#tensor" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/tensors.html">Reference</a></p></li>
</ul>
<section id="whats-in-a-tensor">
<h2><span class="section-number">3.1. </span>What’s in a tensor?<a class="headerlink" href="#whats-in-a-tensor" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Tensors are “just” <strong>multi-dimensional arrays</strong> optimized for fast
computation</p></li>
<li><p>Technically, a <code class="docutils literal notranslate"><span class="pre">tensor</span></code> feels a lot like an <code class="docutils literal notranslate"><span class="pre">R6</span></code> object, in that you
can access its fields and methods using <code class="docutils literal notranslate"><span class="pre">$</span></code>-syntax.</p></li>
</ol>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="m">1</span><span class="p">)</span>
<span class="n">t1</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1
## [ CPUFloatType{1} ]
</pre></div>
</div>
<p>This is a tensor that holds just a single value, 1. * It “lives” on the
<code class="docutils literal notranslate"><span class="pre">CPU</span></code>, and its type is <code class="docutils literal notranslate"><span class="pre">Float</span></code> * <code class="docutils literal notranslate"><span class="pre">{1}</span></code> indicates the tensor shape
instead of the stored value. Here, we have a <strong>one-dimensional</strong>
<code class="docutils literal notranslate"><span class="pre">tensor</span></code>, that is, a <code class="docutils literal notranslate"><span class="pre">vector</span></code>. * We can use the aforementioned
<code class="docutils literal notranslate"><span class="pre">$</span></code>-syntax to individually ascertain these properties</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span><span class="o">$</span><span class="n">dtype</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_Float
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span><span class="o">$</span><span class="n">device</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_device(type=&#39;cpu&#39;)
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span><span class="o">$</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## [1] 1
</pre></div>
</div>
<p>We can also directly change some of these properties, making use of the
tensor object’s <code class="docutils literal notranslate"><span class="pre">$to()</span></code> method:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t1</span><span class="o">$</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">torch_int</span><span class="p">())</span>
<span class="n">t2</span><span class="o">$</span><span class="n">dtype</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_Int
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># only applicable if you have a GPU</span>
<span class="n">t2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t1</span><span class="o">$</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">t2</span><span class="o">$</span><span class="n">device</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_device(type=&#39;cuda&#39;, index=0)
</pre></div>
</div>
<p>Change it’s shape</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t1</span><span class="o">$</span><span class="nf">view</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">))</span>
<span class="n">t3</span><span class="o">$</span><span class="n">shape</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## [1] 1 1
</pre></div>
</div>
<p>Conceptually, this is analogous to a one-element <code class="docutils literal notranslate"><span class="pre">vector</span></code> as well as a
one-element <code class="docutils literal notranslate"><span class="pre">matrix</span></code>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## [1] 1
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">matrix</span><span class="p">(</span><span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>##      [,1]
## [1,]    1
</pre></div>
</div>
</section>
<section id="creating-tensors">
<h2><span class="section-number">3.2. </span>Creating tensors<a class="headerlink" href="#creating-tensors" title="Link to this heading">#</a></h2>
<p>calling <code class="docutils literal notranslate"><span class="pre">torch_tensor()</span></code> and passing in an R value.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">3</span><span class="p">))</span>
<span class="n">t3</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1
##  2
##  3
## [ CPUFloatType{3} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">matrix</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="m">2</span><span class="p">),</span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">))</span>
<span class="n">t4</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1  2
##  1  2
## [ CPUFloatType{2,2} ]
</pre></div>
</div>
<section id="tensors-from-values">
<h3><span class="section-number">3.2.1. </span>Tensors from values<a class="headerlink" href="#tensors-from-values" title="Link to this heading">#</a></h3>
<p>Above, we passed in a one-element vector to torch_tensor(); we can pass
in longer vectors just the same way</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_tensor</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1
##  2
##  3
##  4
##  5
## [ CPULongType{5} ]
</pre></div>
</div>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch</span></code> determines a suitable <code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">type</span></code> itself</p></li>
<li><p>Here, the assumption is that an integer type is desired, and torch
chooses the highest-precision type available (<code class="docutils literal notranslate"><span class="pre">torch_long()</span></code> is
synonymous to <code class="docutils literal notranslate"><span class="pre">torch_int64()</span></code>).</p></li>
<li><p>If we want a floating-point tensor instead, we can use <code class="docutils literal notranslate"><span class="pre">$to()</span></code> on
the newly created instance (as we saw above).</p></li>
</ol>
<ul class="simple">
<li><p>Alternatively, we can just let <code class="docutils literal notranslate"><span class="pre">torch_tensor()</span></code> know right away:</p></li>
</ul>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_tensor</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">torch_float</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1
##  2
##  3
##  4
##  5
## [ CPUFloatType{5} ]
</pre></div>
</div>
<p>Analogously, the default device is the <code class="docutils literal notranslate"><span class="pre">CPU</span></code>; but we can also create a
tensor that, right from the outset, is located on the <code class="docutils literal notranslate"><span class="pre">GPU</span></code>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_tensor</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;cuda&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1
##  2
##  3
##  4
##  5
## [ CUDALongType{5} ]
</pre></div>
</div>
<p>We can pass in an R <code class="docutils literal notranslate"><span class="pre">matrix</span></code> just the same way:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1  4  7
##  2  5  8
##  3  6  9
## [ CPULongType{3,3} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">byrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1  2  3
##  4  5  6
##  7  8  9
## [ CPULongType{3,3} ]
</pre></div>
</div>
<p>What about higher-dimensional data? Following the same principle, we can
pass in an <code class="docutils literal notranslate"><span class="pre">array</span></code>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">array</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">24</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
## (1,.,.) = 
##    1  13
##    5  17
##    9  21
## 
## (2,.,.) = 
##    2  14
##    6  18
##   10  22
## 
## (3,.,.) = 
##    3  15
##    7  19
##   11  23
## 
## (4,.,.) = 
##    4  16
##    8  20
##   12  24
## [ CPULongType{4,3,2} ]
</pre></div>
</div>
<p>Here, pictorially, is the object we created (fig. 3.1). Let’s call the
axis that extends to the right x, the one that goes into the page, y,
and the one that points up, z. Then the tensor extends 4, 3, and 2
units, respectively, in the x, y, and z directions.</p>
<p><img alt="" src="https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/images/tensors-dimensions.png" /></p>
<p>Example for 3 * 2 * 5 <code class="docutils literal notranslate"><span class="pre">tensor</span></code>
<img alt="" src="https://www.tensorflow.org/static/guide/images/tensor/3-axis_front.png" /></p>
<p>Compare that with how the <code class="docutils literal notranslate"><span class="pre">tensor</span></code> prints, above. <code class="docutils literal notranslate"><span class="pre">Array</span></code> and <code class="docutils literal notranslate"><span class="pre">tensor</span></code>
slice the object in different ways. * The <code class="docutils literal notranslate"><span class="pre">tensor</span></code> slices its values
into <code class="docutils literal notranslate"><span class="pre">3x2</span></code> rectangles, extending up and to the back, one for each of the
four x-values. * The <code class="docutils literal notranslate"><span class="pre">array</span></code>, on the other hand, splits them up by
z-value, resulting in two big <code class="docutils literal notranslate"><span class="pre">4x3</span></code> slices that go up and to the right.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">array</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">24</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    1    5    9
## [2,]    2    6   10
## [3,]    3    7   11
## [4,]    4    8   12
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]   13   17   21
## [2,]   14   18   22
## [3,]   15   19   23
## [4,]   16   20   24
</pre></div>
</div>
</section>
<section id="tensors-from-specifications">
<h3><span class="section-number">3.2.2. </span>Tensors from specifications<a class="headerlink" href="#tensors-from-specifications" title="Link to this heading">#</a></h3>
<p>There are two broad conditions when torch’s <code class="docutils literal notranslate"><span class="pre">bulk</span> <span class="pre">creation</span> <span class="pre">functions</span></code>
will come in handy 1. when you don’t care about individual tensor
values, but only about their distribution. 2. They follow some
conventional pattern.</p>
<p>When we use <code class="docutils literal notranslate"><span class="pre">bulk</span> <span class="pre">creation</span> <span class="pre">functions</span></code>, instead of individual values we
specify the <code class="docutils literal notranslate"><span class="pre">shape</span></code> they should have. * Here, for example, we
instantiate a <code class="docutils literal notranslate"><span class="pre">3x3</span></code> tensor, populated with <strong>standard-normally
distributed</strong> values:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_randn</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1.0917  0.2943 -1.2244
##  0.8632  1.0131 -0.8944
##  0.1360 -1.2286 -0.9151
## [ CPUFloatType{3,3} ]
</pre></div>
</div>
<p>And here is the equivalent for values that are <strong>uniformly distributed
between zero and one</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_rand</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  0.9881  0.8841  0.4892
##  0.8545  0.3124  0.4425
##  0.0556  0.1737  0.2220
## [ CPUFloatType{3,3} ]
</pre></div>
</div>
<p>Often, we require tensors of all ones, or all zeroes:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_zeros</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  0  0  0  0  0
##  0  0  0  0  0
## [ CPUFloatType{2,5} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_ones</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1  1
##  1  1
## [ CPUFloatType{2,2} ]
</pre></div>
</div>
<p>let’s see how to create some matrix types that are common in linear
algebra. 1. Here’s an identity matrix:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_eye</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1  0  0  0  0
##  0  1  0  0  0
##  0  0  1  0  0
##  0  0  0  1  0
##  0  0  0  0  1
## [ CPUFloatType{5,5} ]
</pre></div>
</div>
<ol class="arabic simple">
<li><p>Here, a diagonal matrix:</p></li>
</ol>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_diag</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1  0  0
##  0  2  0
##  0  0  3
## [ CPUFloatType{3,3} ]
</pre></div>
</div>
</section>
<section id="tensors-from-datasets">
<h3><span class="section-number">3.2.3. </span>Tensors from datasets<a class="headerlink" href="#tensors-from-datasets" title="Link to this heading">#</a></h3>
<p>First, let’s try <code class="docutils literal notranslate"><span class="pre">JohnsonJohnson</span></code> that comes with base R. * It is a
time series of quarterly earnings per Johnson &amp; Johnson share.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">JohnsonJohnson</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>##       Qtr1  Qtr2  Qtr3  Qtr4
## 1960  0.71  0.63  0.85  0.44
## 1961  0.61  0.69  0.92  0.55
## 1962  0.72  0.77  0.92  0.60
## 1963  0.83  0.80  1.00  0.77
## 1964  0.92  1.00  1.24  1.00
## 1965  1.16  1.30  1.45  1.25
## 1966  1.26  1.38  1.86  1.56
## 1967  1.53  1.59  1.83  1.86
## 1968  1.53  2.07  2.34  2.25
## 1969  2.16  2.43  2.70  2.25
## 1970  2.79  3.42  3.69  3.60
## 1971  3.60  4.32  4.32  4.05
## 1972  4.86  5.04  5.04  4.41
## 1973  5.58  5.85  6.57  5.31
## 1974  6.03  6.39  6.93  5.85
## 1975  6.93  7.74  7.83  6.12
## 1976  7.74  8.91  8.28  6.84
## 1977  9.54 10.26  9.54  8.73
## 1978 11.88 12.06 12.15  8.91
## 1979 14.04 12.96 14.85  9.99
## 1980 16.20 14.67 16.02 11.61
</pre></div>
</div>
<p>If we just pass it directly to <code class="docutils literal notranslate"><span class="pre">torch_tensor()</span></code>, we will get a vector,
similar to <code class="docutils literal notranslate"><span class="pre">unclass(JohnsonJohnson)</span></code></p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">torch_tensor</span><span class="p">(</span><span class="n">JohnsonJohnson</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##   0.7100
##   0.6300
##   0.8500
##   0.4400
##   0.6100
##   0.6900
##   0.9200
##   0.5500
##   0.7200
##   0.7700
##   0.9200
##   0.6000
##   0.8300
##   0.8000
##   1.0000
##   0.7700
##   0.9200
##   1.0000
##   1.2400
##   1.0000
##   1.1600
##   1.3000
##   1.4500
##   1.2500
##   1.2600
##   1.3800
##   1.8600
##   1.5600
##   1.5300
##   1.5900
## ... [the output was truncated (use n=-1 to disable)]
## [ CPUFloatType{84} ]
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch</span></code> can only work with what it is given; and here, what it is given
is actually a <code class="docutils literal notranslate"><span class="pre">vector</span></code> of <code class="docutils literal notranslate"><span class="pre">doubles</span></code> arranged in quarterly order. The
data just print the way they do because they are of class <code class="docutils literal notranslate"><span class="pre">ts</span></code>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">unclass</span><span class="p">(</span><span class="n">JohnsonJohnson</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>##  [1]  0.71  0.63  0.85  0.44  0.61  0.69  0.92  0.55  0.72  0.77  0.92  0.60
## [13]  0.83  0.80  1.00  0.77  0.92  1.00  1.24  1.00  1.16  1.30  1.45  1.25
## [25]  1.26  1.38  1.86  1.56  1.53  1.59  1.83  1.86  1.53  2.07  2.34  2.25
## [37]  2.16  2.43  2.70  2.25  2.79  3.42  3.69  3.60  3.60  4.32  4.32  4.05
## [49]  4.86  5.04  5.04  4.41  5.58  5.85  6.57  5.31  6.03  6.39  6.93  5.85
## [61]  6.93  7.74  7.83  6.12  7.74  8.91  8.28  6.84  9.54 10.26  9.54  8.73
## [73] 11.88 12.06 12.15  8.91 14.04 12.96 14.85  9.99 16.20 14.67 16.02 11.61
## attr(,&quot;tsp&quot;)
## [1] 1960.00 1980.75    4.00
</pre></div>
</div>
</section>
</section>
<section id="operations-on-tensors">
<h2><span class="section-number">3.3. </span>Operations on tensors<a class="headerlink" href="#operations-on-tensors" title="Link to this heading">#</a></h2>
<p>We can perform all the usual mathematical operations on tensors.: <code class="docutils literal notranslate"><span class="pre">add</span></code>,
<code class="docutils literal notranslate"><span class="pre">subtract</span></code>, <code class="docutils literal notranslate"><span class="pre">divide</span></code> …</p>
<ul class="simple">
<li><p>These operations are available as functions (starting with <code class="docutils literal notranslate"><span class="pre">torch_</span></code>)
as well as as methods on objects (invoked with <code class="docutils literal notranslate"><span class="pre">$</span></code>-syntax).</p></li>
</ul>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">))</span>
<span class="n">t2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">))</span>

<span class="nf">torch_add</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span><span class="w"> </span><span class="n">t2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  4
##  6
## [ CPUFloatType{2} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># equivalently</span>
<span class="n">t1</span><span class="o">$</span><span class="nf">add</span><span class="p">(</span><span class="n">t2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  4
##  6
## [ CPUFloatType{2} ]
</pre></div>
</div>
<p>In both cases, a new object is created; neither <code class="docutils literal notranslate"><span class="pre">t1</span></code> nor <code class="docutils literal notranslate"><span class="pre">t2</span></code> are
modified. There exists an alternate method that <strong>modifies its object
in-place</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1
##  2
## [ CPUFloatType{2} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span><span class="o">$</span><span class="nf">add_</span><span class="p">(</span><span class="n">t2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  4
##  6
## [ CPUFloatType{2} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  4
##  6
## [ CPUFloatType{2} ]
</pre></div>
</div>
<p>In fact, the same pattern applies for other operations: Whenever you see
an <strong>underscore</strong> appended, the object is <strong>modified in-place</strong>.</p>
<p>Let’s start with the <code class="docutils literal notranslate"><span class="pre">dot</span> <span class="pre">product</span></code> of two one-dimensional structures,
i.e., <code class="docutils literal notranslate"><span class="pre">vectors</span></code>.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">)</span>
<span class="n">t2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="m">4</span><span class="o">:</span><span class="m">6</span><span class="p">)</span>
<span class="n">t1</span><span class="o">$</span><span class="nf">dot</span><span class="p">(</span><span class="n">t2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
## 32
## [ CPULongType{} ]
</pre></div>
</div>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch</span></code> does not distinguish between <code class="docutils literal notranslate"><span class="pre">row</span> <span class="pre">vectors</span></code> and
<code class="docutils literal notranslate"><span class="pre">column</span> <span class="pre">vectors</span></code>.</p></li>
<li><p>if we multiply a <code class="docutils literal notranslate"><span class="pre">vector</span></code> with a <code class="docutils literal notranslate"><span class="pre">matrix</span></code>, using <code class="docutils literal notranslate"><span class="pre">torch_matmul()</span></code>,
we don’t need to worry about the vector’s orientation either:</p></li>
</ol>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">12</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">byrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">))</span>
<span class="n">t3</span><span class="o">$</span><span class="nf">matmul</span><span class="p">(</span><span class="n">t1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  14
##  32
##  50
##  68
## [ CPULongType{4} ]
</pre></div>
</div>
<p>The same function, <code class="docutils literal notranslate"><span class="pre">torch_matmul(</span></code>), would be used to multiply two
matrices. Note how this is different from what <code class="docutils literal notranslate"><span class="pre">torch_multiply()</span></code> does,
namely, <strong>scalar-multiply</strong> its arguments:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">)</span>
<span class="n">t2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="m">4</span><span class="o">:</span><span class="m">6</span><span class="p">)</span>
<span class="nf">torch_multiply</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span><span class="w"> </span><span class="n">t2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##   4
##  10
##  18
## [ CPULongType{3} ]
</pre></div>
</div>
<section id="summary-operations">
<h3><span class="section-number">3.3.1. </span>Summary operations<a class="headerlink" href="#summary-operations" title="Link to this heading">#</a></h3>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">outer</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">6</span><span class="p">)</span>
<span class="n">m</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    2    3    4    5    6
## [2,]    2    4    6    8   10   12
## [3,]    3    6    9   12   15   18
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">sum</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## [1] 126
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">apply</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">sum</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## [1] 21 42 63
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">apply</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">sum</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## [1]  6 12 18 24 30 36
</pre></div>
</div>
<p>And now, the <code class="docutils literal notranslate"><span class="pre">torch</span></code> equivalents. We start with the <strong>overall</strong> <code class="docutils literal notranslate"><span class="pre">sum.</span></code></p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_outer</span><span class="p">(</span><span class="nf">torch_tensor</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">3</span><span class="p">),</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">6</span><span class="p">))</span>
<span class="n">t</span><span class="o">$</span><span class="nf">sum</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
## 126
## [ CPULongType{} ]
</pre></div>
</div>
<p>It gets more interesting for the row and column sums. The <code class="docutils literal notranslate"><span class="pre">dim</span></code> argument
tells <code class="docutils literal notranslate"><span class="pre">torch</span></code> which <code class="docutils literal notranslate"><span class="pre">dimension(s)</span></code> to sum over. Passing in <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">=</span> <span class="pre">1</span></code>, we
see:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="o">$</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##   6
##  12
##  18
##  24
##  30
##  36
## [ CPULongType{6} ]
</pre></div>
</div>
<p>Unexpectedly, these are the <code class="docutils literal notranslate"><span class="pre">column</span> <span class="pre">sums</span></code>! Before drawing conclusions,
let’s check what happens with <code class="docutils literal notranslate"><span class="pre">dim</span> <span class="pre">=</span> <span class="pre">2</span></code>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="o">$</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  21
##  42
##  63
## [ CPULongType{3} ]
</pre></div>
</div>
<p>Instead, the conceptual difference is specific to <code class="docutils literal notranslate"><span class="pre">aggregating</span></code>, or
<code class="docutils literal notranslate"><span class="pre">grouping</span></code>, operations. In R, <code class="docutils literal notranslate"><span class="pre">grouping</span></code>, in fact, nicely characterizes
what we have in mind: We group by row (dimension 1) for row summaries,
by column (dimension 2) for column summaries. In torch, the thinking is
different: We <strong>collapse</strong> the columns (dimension 2) to compute row
summaries, the rows (dimension 1) for column summaries.</p>
<p>The same thinking applies in higher dimensions. Assume, for example,
that we been recording time series data for four individuals. There are
two features, and both of them have been measured at three times. If we
were planning to train a recurrent neural network (much more on that
later), we would arrange the measurements like so:</p>
<ul class="simple">
<li><p>Dimension 1: Runs over individuals.</p></li>
<li><p>Dimension 2: Runs over points in time.</p></li>
<li><p>Dimension 3: Runs over features.</p></li>
</ul>
<p>The tensor then would look like this:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_randn</span><span class="p">(</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)</span>
<span class="n">t</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
## (1,.,.) = 
##   1.4176  0.6129
##  -0.7762 -0.2163
##  -0.1870  0.0977
## 
## (2,.,.) = 
##  -0.0486 -1.6754
##  -0.7790 -1.4072
##   1.7094  0.6100
## 
## (3,.,.) = 
##  -0.6665  0.7684
##   0.0685  0.2067
##   0.9274  0.3285
## 
## (4,.,.) = 
##  -2.5222 -0.6573
##  -0.1464  0.1550
##   0.9306 -0.3727
## [ CPUFloatType{4,3,2} ]
</pre></div>
</div>
<p>To obtain feature averages, independently of subject and time, we would
<strong>collapse</strong> dimensions 1 and 2:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="o">$</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
## 0.001 *
## -6.0413
## -129.1528
## [ CPUFloatType{2} ]
</pre></div>
</div>
<p>If, on the other hand, we wanted feature averages, but individually per
person, we’d do:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="o">$</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  0.1515  0.1648
##  0.2939 -0.8242
##  0.1098  0.4345
## -0.5793 -0.2917
## [ CPUFloatType{4,2} ]
</pre></div>
</div>
</section>
</section>
<section id="accessing-parts-of-a-tensor">
<h2><span class="section-number">3.4. </span>Accessing parts of a tensor<a class="headerlink" href="#accessing-parts-of-a-tensor" title="Link to this heading">#</a></h2>
<section id="indexing-and-slicing">
<h3><span class="section-number">3.4.1. </span>Indexing and Slicing<a class="headerlink" href="#indexing-and-slicing" title="Link to this heading">#</a></h3>
<p>In the below example, we ask for the first column of a two-dimensional
<code class="docutils literal notranslate"><span class="pre">tensor</span></code>; the result is one-dimensional, i.e., a <code class="docutils literal notranslate"><span class="pre">vector</span></code>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">byrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">))</span>
<span class="n">t</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1  2  3
##  4  5  6
##  7  8  9
## [ CPULongType{3,3} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="p">[</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1
##  2
##  3
## [ CPULongType{3} ]
</pre></div>
</div>
<p>If we specify <code class="docutils literal notranslate"><span class="pre">drop</span> <span class="pre">=</span> <span class="pre">FALSE</span></code>, though, <strong>dimensionality is preserved</strong>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="p">[</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">drop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1  2  3
## [ CPULongType{1,3} ]
</pre></div>
</div>
<p>When <code class="docutils literal notranslate"><span class="pre">slicing</span></code>, there are no singleton dimensions – and thus, no
additional considerations to be taken into account:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_rand</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)</span>
<span class="n">t</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
## (1,.,.) = 
##   0.3448  0.2467  0.2844
##   0.0130  0.0119  0.4501
##   0.4605  0.1153  0.9454
## 
## (2,.,.) = 
##   0.6728  0.4534  0.6797
##   0.5317  0.6467  0.8967
##   0.1564  0.0813  0.7384
## 
## (3,.,.) = 
##   0.0476  0.1795  0.0142
##   0.8283  0.2712  0.2839
##   0.3849  0.6851  0.2013
## [ CPUFloatType{3,3,3} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="o">:</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">)]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
## (1,.,.) = 
##   0.0130  0.4501
##   0.4605  0.9454
## 
## (2,.,.) = 
##   0.5317  0.8967
##   0.1564  0.7384
## [ CPUFloatType{2,2,2} ]
</pre></div>
</div>
</section>
<section id="advanced-query">
<h3><span class="section-number">3.4.2. </span>Advanced query<a class="headerlink" href="#advanced-query" title="Link to this heading">#</a></h3>
<p>One of these extensions concerns accessing the last element in a
<code class="docutils literal notranslate"><span class="pre">tensor</span></code>. Conveniently, in torch, we can use <code class="docutils literal notranslate"><span class="pre">-1</span></code> to accomplish that:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">byrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">))</span>
<span class="n">t</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1  2
##  3  4
## [ CPULongType{2,2} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="p">[</span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="m">-1</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
## 4
## [ CPULongType{} ]
</pre></div>
</div>
<p>Another useful feature extends <code class="docutils literal notranslate"><span class="pre">slicing</span></code> syntax to allow for a <strong>step
pattern</strong>, to be specified after a second colon. Here, we request values
from every second column between columns one and eight:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">byrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">))</span>
<span class="n">t</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##   1   2   3   4   5   6   7   8   9  10
##  11  12  13  14  15  16  17  18  19  20
## [ CPULongType{2,10} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="p">[</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">8</span><span class="o">:</span><span class="m">2</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##   1   3   5   7
##  11  13  15  17
## [ CPULongType{2,4} ]
</pre></div>
</div>
<p>Finally, sometimes the same code should be able to work with tensors of
<strong>different dimensionalities</strong>.</p>
<p>In this case, we can use <code class="docutils literal notranslate"><span class="pre">..</span></code> to collectively designate any existing
dimensions not explicitly referenced.</p>
<p>For example, say we want to index into the first dimension of whatever
tensor is passed, be it a <code class="docutils literal notranslate"><span class="pre">matrix</span></code>, an <code class="docutils literal notranslate"><span class="pre">array</span></code>, or some
higher-dimensional structure. The following</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_randn</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)</span>
<span class="n">t2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_randn</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)</span>
<span class="n">t3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_randn</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">)</span>
<span class="n">t1</span><span class="p">[</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">..</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1.1376
##  2.3105
## [ CPUFloatType{2} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t2</span><span class="p">[</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">..</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
## -1.1752 -0.3022
##  0.8728  0.2683
## [ CPUFloatType{2,2} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t3</span><span class="p">[</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">..</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
## (1,.,.) = 
##  -0.0562  1.2800
##  -0.1790 -0.8129
## 
## (2,.,.) = 
##   1.0048 -2.3087
##  -1.4101  0.1139
## [ CPUFloatType{2,2,2} ]
</pre></div>
</div>
<p>If we wanted to index into the last dimension instead, we’d write
<code class="docutils literal notranslate"><span class="pre">t[..,</span> <span class="pre">1]</span></code>. We can even combine both:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t3</span><span class="p">[</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">..</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  1.2800 -0.8129
## -2.3087  0.1139
## [ CPUFloatType{2,2} ]
</pre></div>
</div>
</section>
</section>
<section id="reshaping-tensors">
<h2><span class="section-number">3.5. </span>Reshaping tensors<a class="headerlink" href="#reshaping-tensors" title="Link to this heading">#</a></h2>
<p>We can modify a tensor’s shape, without juggling around its values,
using the <code class="docutils literal notranslate"><span class="pre">view()</span></code> method</p>
<p>Here is the initial tensor, a vector of length 24:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_zeros</span><span class="p">(</span><span class="m">24</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  0
##  0
##  0
## ... [the output was truncated (use n=-1 to disable)]
## [ CPUFloatType{24} ]
</pre></div>
</div>
<p>Here is that same <code class="docutils literal notranslate"><span class="pre">vector</span></code>, reshaped to a wide <code class="docutils literal notranslate"><span class="pre">matrix</span></code>:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="o">$</span><span class="nf">view</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">12</span><span class="p">))</span>
<span class="n">t2</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  0  0  0  0  0  0  0  0  0  0  0  0
##  0  0  0  0  0  0  0  0  0  0  0  0
## [ CPUFloatType{2,12} ]
</pre></div>
</div>
<p>Whenever we ask <code class="docutils literal notranslate"><span class="pre">torch</span></code> to perform an operation that changes the shape
of a tensor, it tries to fulfill the request <strong>without allocating new
storage</strong> for the tensor’s contents.</p>
<p>How does <code class="docutils literal notranslate"><span class="pre">torch</span></code> do it? Let’s see a concrete example. We start with a 3
x 5 <code class="docutils literal notranslate"><span class="pre">matrix</span></code>.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_tensor</span><span class="p">(</span><span class="nf">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">byrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">))</span>
<span class="n">t</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##   1   2   3   4   5
##   6   7   8   9  10
##  11  12  13  14  15
## [ CPULongType{3,5} ]
</pre></div>
</div>
<p>Tensors have a <code class="docutils literal notranslate"><span class="pre">stride()</span></code> method that tracks, for every dimension, how
many elements have to be traversed to arrive at its next element.</p>
<p>For the above tensor <code class="docutils literal notranslate"><span class="pre">t</span></code>, to go to the next <code class="docutils literal notranslate"><span class="pre">row</span></code>, we have to skip over
five elements, while to go to the next <code class="docutils literal notranslate"><span class="pre">column</span></code>, we need to skip just
one:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t</span><span class="o">$</span><span class="nf">stride</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## [1] 5 1
</pre></div>
</div>
<p>Now we reshape the tensor so it has five <code class="docutils literal notranslate"><span class="pre">rows</span></code> and three <code class="docutils literal notranslate"><span class="pre">columns</span></code>
instead. Remember, the data themselves do not change.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="o">$</span><span class="nf">view</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">))</span>
<span class="n">t2</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##   1   2   3
##   4   5   6
##   7   8   9
##  10  11  12
##  13  14  15
## [ CPULongType{5,3} ]
</pre></div>
</div>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t2</span><span class="o">$</span><span class="nf">stride</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## [1] 3 1
</pre></div>
</div>
<p>This time, to arrive at the next row, we just skip three elements
instead of five. To get to the next column, we still just “jump over” a
single element only:</p>
</section>
<section id="broadcasting">
<h2><span class="section-number">3.6. </span>Broadcasting<a class="headerlink" href="#broadcasting" title="Link to this heading">#</a></h2>
<p>Multiply every element by a <code class="docutils literal notranslate"><span class="pre">scalar</span></code>. This works:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">t1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">torch_randn</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">)</span>
<span class="n">t1</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">0.5</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>## torch_tensor
##  0.0130 -0.0835  0.6924  0.6262  0.1971
## -0.5043 -0.9089  0.1011 -0.4738 -0.6423
##  0.2129  0.3098 -0.3925  0.3697 -0.2689
## [ CPUFloatType{3,5} ]
</pre></div>
</div>
<p>Add the same vector to every row in a matrix</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">byrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>
<span class="n">m2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">matrix</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">ncol</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="n">byrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>

<span class="c1"># m + m2</span>
<span class="c1">## Error in m + m2 : non-conformable arrays</span>
</pre></div>
</div>
<p>Neither does it help if we make <code class="docutils literal notranslate"><span class="pre">m2</span></code> a <code class="docutils literal notranslate"><span class="pre">vector</span></code>.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">m3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">5</span>

<span class="n">m</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">m3</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    2    6    5    9    8
## [2,]    8   12   11   10   14
## [3,]   14   13   17   16   20
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book\torch"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="02-tensorflow-installation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2. </span><code class="docutils literal notranslate"><span class="pre">Keras</span></code> installation</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#whats-in-a-tensor">3.1. What’s in a tensor?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-tensors">3.2. Creating tensors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors-from-values">3.2.1. Tensors from values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors-from-specifications">3.2.2. Tensors from specifications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors-from-datasets">3.2.3. Tensors from datasets</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operations-on-tensors">3.3. Operations on tensors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-operations">3.3.1. Summary operations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-parts-of-a-tensor">3.4. Accessing parts of a tensor</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-and-slicing">3.4.1. Indexing and Slicing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-query">3.4.2. Advanced query</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reshaping-tensors">3.5. Reshaping tensors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting">3.6. Broadcasting</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yalin Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>