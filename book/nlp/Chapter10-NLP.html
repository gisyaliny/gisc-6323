
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>17. Natural Language Processing &#8212; Machine Learning for Socio-Economic and Georeferenced Data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'book/nlp/Chapter10-NLP';</script>
    <link rel="canonical" href="https://gisyaliny.github.io/gisc-6323/book/nlp/Chapter10-NLP.html" />
    <link rel="icon" href="../../_static/fav.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="18. Recurrent Neural Networks" href="Chapter10-RNN.html" />
    <link rel="prev" title="16. Convolution Neural Network" href="../cv/Chapter10-CNN.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Machine Learning for Socio-Economic and Georeferenced Data - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Machine Learning for Socio-Economic and Georeferenced Data - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction &amp; Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Intro/author.html">About the Author</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro/preface.html">Preface</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Traditional statistics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../tranditional/Chapter2-Statistical-Learning.html">1. Statistical Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tranditional/Chapter3-Linear-Regression.html">2. Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tranditional/Chapter4-Classification.html">3. Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tranditional/Chapter5-Sampling.html">4. Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tranditional/Chapter6-Model-Selection.html">5. Model Selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tranditional/Chapter6-Dimention-Reduction.html">6. Dimension Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tranditional/Chapter7-non-linear-regression.html">7. Non-linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tranditional/Chapter4-Logistic-Regression.html">8. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tranditional/Chapter8-Tree-Based-Methods.html">9. Tree-Based Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tranditional/Chapter9-Support-Vector-Machines.html">10. Support Vector Machines (SVM)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning Basic</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../torch/01-torch-installation.html">11. <code class="docutils literal notranslate"><span class="pre">torch</span></code> Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch/02-tensorflow-installation.html">12. <code class="docutils literal notranslate"><span class="pre">Keras</span></code> installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch/03-Tensor.html">13. Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch/04-SGD.html">14. Gradient Descent (GD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torch/05-Autograd.html">15. Autograd</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Vision</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../cv/Chapter10-CNN.html">16. Convolution Neural Network</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Natural Language Processing</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">17. Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="Chapter10-RNN.html">18. Recurrent Neural Networks</a></li>




</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/gisyaliny/gisc-6323/master?urlpath=tree/book/nlp/Chapter10-NLP.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/gisyaliny/gisc-6323/blob/master/book/nlp/Chapter10-NLP.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gisyaliny/gisc-6323" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gisyaliny/gisc-6323/edit/main/book/nlp/Chapter10-NLP.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gisyaliny/gisc-6323/issues/new?title=Issue%20on%20page%20%2Fbook/nlp/Chapter10-NLP.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/book/nlp/Chapter10-NLP.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Natural Language Processing</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-to-matrix-vectors">17.1. Word to Matrix / Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words">17.1.1. Bag-of-Words</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#drawbacks">17.1.1.1. Drawbacks</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-grams">17.1.2. N-grams</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages">17.1.2.1. Advantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">17.1.2.2. Drawbacks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mitigation-strategies">17.1.2.3. Mitigation Strategies</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf">17.2. TF-IDF</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#term-frequency-tf">17.2.1. Term Frequency (TF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse-document-frequency-idf">17.2.2. Inverse Document Frequency (IDF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">17.2.3. Drawbacks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">17.2.4. Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embeddings-word2vec-fasttext">17.3. Word Embeddings (Word2Vec, FastText)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">17.3.1. Word2Vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fasttext">17.3.2. FastText</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">17.3.3. Drawbacks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#glove-global-vectors-for-word-representation">17.4. GloVe (Global Vectors for Word Representation)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#design-purpose">17.4.1. Design Purpose:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantage">17.4.2. Advantage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">17.4.3. Drawbacks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-co-occurrence-matrix">17.4.4. Example of co-occurrence matrix</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contextualized-word-embeddings-elmo-and-bert">17.5. Contextualized word embeddings (ELMo and BERT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elmo-embeddings-from-language-models">17.5.1. ELMo (Embeddings from Language Models)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bert-bidirectional-encoder-representations-from-transformers">17.5.2. BERT (Bidirectional Encoder Representations from Transformers)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-lda-lsa">17.6. Topic Modeling (LDA, LSA)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-dirichlet-allocation-lda">17.6.1. Latent Dirichlet Allocation (LDA)</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="natural-language-processing">
<h1><span class="section-number">17. </span>Natural Language Processing<a class="headerlink" href="#natural-language-processing" title="Link to this heading">#</a></h1>
<section id="word-to-matrix-vectors">
<h2><span class="section-number">17.1. </span>Word to Matrix / Vectors<a class="headerlink" href="#word-to-matrix-vectors" title="Link to this heading">#</a></h2>
<p>Documents have different length, and consist of sequences of words. How do we create features X to characterize a document?</p>
<section id="bag-of-words">
<h3><span class="section-number">17.1.1. </span>Bag-of-Words<a class="headerlink" href="#bag-of-words" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>From a dictionary, identify the 10K most frequently occurring words.</p></li>
<li><p>Create a binary vector of length <span class="math notranslate nohighlight">\(p=10K\)</span> for each document, and score a 1 in every position that the corresponding word occurred.</p>
<ul>
<li><p>With <span class="math notranslate nohighlight">\(n\)</span> documents, we now have a <span class="math notranslate nohighlight">\(n*p\)</span> <strong>sparse</strong> feature matrix X.</p></li>
</ul>
</li>
</ul>
<section id="drawbacks">
<h4><span class="section-number">17.1.1.1. </span>Drawbacks<a class="headerlink" href="#drawbacks" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Bag-of-Words</span> <span class="pre">(BoW)</span></code> <strong>does not consider the sequence of words</strong> in the text. It consider words independently, we call it <code class="docutils literal notranslate"><span class="pre">unigrams</span></code>. We can instead use <code class="docutils literal notranslate"><span class="pre">bigrams</span></code> (occurrences of adjacent word pairs), and in general <code class="docutils literal notranslate"><span class="pre">n-grams</span></code>.</p></li>
</ul>
<p><img alt="image.png" src="../../_images/011.png" /></p>
</section>
</section>
<section id="n-grams">
<h3><span class="section-number">17.1.2. </span>N-grams<a class="headerlink" href="#n-grams" title="Link to this heading">#</a></h3>
<p>N-grams are contiguous sequences of n items (words, characters, or tokens) extracted from a text corpus. In the context of natural language processing (NLP), N-grams are commonly used to capture patterns and dependencies between words in a sequence of text.</p>
<section id="advantages">
<h4><span class="section-number">17.1.2.1. </span>Advantages<a class="headerlink" href="#advantages" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Capturing Context</strong>: N-grams preserve some level of word order and context, allowing models to capture dependencies between adjacent words.</p></li>
<li><p><strong>Flexibility</strong>: N-grams can be adjusted to different granularities (<code class="docutils literal notranslate"><span class="pre">unigrams</span></code>, <code class="docutils literal notranslate"><span class="pre">bigrams</span></code>, <code class="docutils literal notranslate"><span class="pre">trigrams</span></code>, etc.), providing flexibility in capturing different levels of context.</p></li>
</ul>
</section>
<section id="id1">
<h4><span class="section-number">17.1.2.2. </span>Drawbacks<a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Data Sparsity:</strong> As the length of the n-grams increases, the number of unique combinations grows exponentially, leading to sparsity issues, especially with smaller datasets.</p></li>
<li><p><strong>Lack of Generalization</strong>: N-grams may <strong>overfit</strong> to specific patterns present in the training data, making them less generalizable to unseen data.</p></li>
</ul>
</section>
<section id="mitigation-strategies">
<h4><span class="section-number">17.1.2.3. </span>Mitigation Strategies<a class="headerlink" href="#mitigation-strategies" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Pruning</strong>: Limit the vocabulary size or discard low-frequency n-grams to reduce computational complexity.</p></li>
<li><p><strong>Smoothing</strong>: Address data sparsity issues by smoothing probabilities of unseen n-grams.</p></li>
</ul>
<p><img alt="image.png" src="../../_images/021.png" /></p>
</section>
</section>
</section>
<section id="tf-idf">
<h2><span class="section-number">17.2. </span>TF-IDF<a class="headerlink" href="#tf-idf" title="Link to this heading">#</a></h2>
<ul>
<li><p><a class="reference external" href="https://www.kdnuggets.com/2022/09/convert-text-documents-tfidf-matrix-tfidfvectorizer.html">Reference</a></p>
<div class="math notranslate nohighlight">
\[
  TF-IDF(t,d,D) = TF(t,d) * IDF(t,D)
  \]</div>
</li>
</ul>
<section id="term-frequency-tf">
<h3><span class="section-number">17.2.1. </span>Term Frequency (TF)<a class="headerlink" href="#term-frequency-tf" title="Link to this heading">#</a></h3>
<p>Measures how frequently a term (word) occurs in a document.</p>
<ul class="simple">
<li><p>It is calculated as the ratio of the number of times a term appears in a document to the total number of terms in the document.
$<span class="math notranslate nohighlight">\(
TF(t,d) = \frac{Number\: of\: occurrences\: of\: term\: t\: in\: document\: d}{Total\: number\: of\: terms\: in\: document\: d}
\)</span>$</p></li>
</ul>
</section>
<section id="inverse-document-frequency-idf">
<h3><span class="section-number">17.2.2. </span>Inverse Document Frequency (IDF)<a class="headerlink" href="#inverse-document-frequency-idf" title="Link to this heading">#</a></h3>
<p>Measures <strong>how important a term is</strong> across the entire collection of documents.</p>
<ul>
<li><p>It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term, with a smoothing term to avoid division by zero.</p>
<div class="math notranslate nohighlight">
\[
  IDF(t,D) = log(\frac{Number\: of \: documents}{Number\: of\: documents\: containing\: term\: t})
  \]</div>
</li>
<li><p><strong>Text A</strong>: Jupiter is the largest planet</p></li>
<li><p><strong>Text B</strong>: Mars is the fourth planet from the sun</p></li>
</ul>
<p><img alt="image.png" src="../../_images/031.png" /></p>
</section>
<section id="id2">
<h3><span class="section-number">17.2.3. </span>Drawbacks<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">BoW</span></code> and <code class="docutils literal notranslate"><span class="pre">TF-IDF</span></code> represent words as sparse vectors of counts or weighted frequencies, respectively, without <strong>capturing semantic relationships between words</strong>.</p></li>
<li><p>They may <strong>struggle to handle out-of-vocabulary words</strong> and fail to capture subtle semantic similarities between words.</p></li>
</ul>
</section>
<section id="example">
<h3><span class="section-number">17.2.4. </span>Example<a class="headerlink" href="#example" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In this tutorial, we are going to use <code class="docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code> from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> to convert the text and view the <code class="docutils literal notranslate"><span class="pre">TF-IDF</span></code> matrix.</p></li>
<li><p>In the code below, we have a small corpus of 4 documents.</p>
<ul>
<li><p>First, we will create a vectorizer object using <code class="docutils literal notranslate"><span class="pre">TfidfVectorizer()</span></code> and fit and transform the text data into vectors.</p></li>
<li><p>After that, we will use vectorizers to extract the names of the words.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
          <span class="s1">&#39;KDnuggets Collection of data science Projects&#39;</span><span class="p">,</span>
          <span class="s1">&#39;3 Free Statistics Courses for data science&#39;</span><span class="p">,</span>
          <span class="s1">&#39;Parallel Processing Large File in Python&#39;</span><span class="p">,</span>
          <span class="s1">&#39;15 Python Coding Interview Questions You Must Know For data science&#39;</span><span class="p">,</span>
 <span class="p">]</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>

<span class="c1"># TD-IDF Matrix</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="c1"># extracting feature names</span>
<span class="n">tfidf_tokens</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We will now use TF-IDF tokens and vectors to create a pandas dataframe.</p>
<ol class="arabic simple">
<li><p>Convert the vectors to arrays and add it to the data argument.</p></li>
<li><p>Four indexes are created manually.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tfidf_tokens</span></code> names are added to columns</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> 
    <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Doc1&quot;</span><span class="p">,</span> <span class="s2">&quot;Doc2&quot;</span><span class="p">,</span> <span class="s2">&quot;Doc3&quot;</span><span class="p">,</span> <span class="s2">&quot;Doc4&quot;</span><span class="p">],</span> 
    <span class="n">columns</span><span class="o">=</span><span class="n">tfidf_tokens</span>
<span class="p">)</span>

<span class="n">result</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>15</th>
      <th>coding</th>
      <th>collection</th>
      <th>courses</th>
      <th>data</th>
      <th>file</th>
      <th>for</th>
      <th>free</th>
      <th>in</th>
      <th>interview</th>
      <th>...</th>
      <th>must</th>
      <th>of</th>
      <th>parallel</th>
      <th>processing</th>
      <th>projects</th>
      <th>python</th>
      <th>questions</th>
      <th>science</th>
      <th>statistics</th>
      <th>you</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Doc1</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.455732</td>
      <td>0.000000</td>
      <td>0.290888</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.455732</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.455732</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.290888</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Doc2</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.474771</td>
      <td>0.303040</td>
      <td>0.000000</td>
      <td>0.374315</td>
      <td>0.474771</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.303040</td>
      <td>0.474771</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Doc3</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.421765</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.421765</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.421765</td>
      <td>0.421765</td>
      <td>0.000000</td>
      <td>0.332524</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>Doc4</th>
      <td>0.332264</td>
      <td>0.332264</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.212080</td>
      <td>0.000000</td>
      <td>0.261961</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.332264</td>
      <td>...</td>
      <td>0.332264</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.261961</td>
      <td>0.332264</td>
      <td>0.212080</td>
      <td>0.000000</td>
      <td>0.332264</td>
    </tr>
  </tbody>
</table>
<p>4 rows × 23 columns</p>
</div></div></div>
</div>
</section>
</section>
<section id="word-embeddings-word2vec-fasttext">
<h2><span class="section-number">17.3. </span>Word Embeddings (Word2Vec, FastText)<a class="headerlink" href="#word-embeddings-word2vec-fasttext" title="Link to this heading">#</a></h2>
<ul>
<li><p><a class="reference external" href="https://kavita-ganesan.com/fasttext-vs-word2vec/">Reference</a></p>
<p>Word Embeddings are dense vector representations of words in a continuous vector space, where <strong>words with similar meanings are represented by vectors that are close together</strong>.</p>
</li>
</ul>
<p><img alt="image.png" src="../../_images/041.png" /></p>
<section id="word2vec">
<h3><span class="section-number">17.3.1. </span>Word2Vec<a class="headerlink" href="#word2vec" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Word2Vec</span></code>, introduced by Mikolov et al. in 2013, aims to learn word embeddings by predicting the context of words in a large corpus. It uses shallow neural networks to learn distributed representations of words based on their co-occurrence patterns.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Word2Vec</span></code> uses a <code class="docutils literal notranslate"><span class="pre">softmax</span></code> function and <code class="docutils literal notranslate"><span class="pre">negative</span> <span class="pre">sampling</span></code> or <code class="docutils literal notranslate"><span class="pre">hierarchical</span> <span class="pre">softmax</span></code> to train the neural network efficiently.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Word2Vec</span></code> consists of two primary architectures: <code class="docutils literal notranslate"><span class="pre">Continuous</span> <span class="pre">Bag</span> <span class="pre">of</span> <span class="pre">Words</span> <span class="pre">(CBOW)</span></code> and <code class="docutils literal notranslate"><span class="pre">Skip-gram</span></code>.</p>
<ul>
<li><p>In <code class="docutils literal notranslate"><span class="pre">CBOW</span></code>, the model predicts the target word based on its context words</p></li>
<li><p>In <code class="docutils literal notranslate"><span class="pre">Skip-gram</span></code>, the model predicts context words given the target word.</p></li>
</ul>
</li>
</ul>
<p><img alt="image.png" src="../../_images/051.png" /></p>
</section>
<section id="fasttext">
<h3><span class="section-number">17.3.2. </span>FastText<a class="headerlink" href="#fasttext" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">FastText</span></code>, also developed by Mikolov et al., extends <code class="docutils literal notranslate"><span class="pre">Word2Vec</span></code> by <strong>considering subword information</strong>. It learns embeddings <strong>not only for complete words but also for character n-grams</strong>, allowing it to capture morphological information and handle out-of-vocabulary words better.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FastText</span></code> extends <code class="docutils literal notranslate"><span class="pre">Word2Vec</span></code> by representing words as the sum of the embeddings of their <code class="docutils literal notranslate"><span class="pre">character</span> <span class="pre">n-grams</span></code>.</p>
<ul>
<li><p>A <code class="docutils literal notranslate"><span class="pre">character</span> <span class="pre">n-gram</span></code> is a set of co-occurring characters within a given window. It’s very similar to <code class="docutils literal notranslate"><span class="pre">word</span> <span class="pre">n-grams</span></code>, only that the window size is at the character level.</p></li>
<li><p>It learns embeddings for <strong>both words and character n-grams separately and combines them to generate the final word embeddings</strong>.</p></li>
</ul>
</li>
</ul>
<p><img alt="image.png" src="../../_images/061.png" /></p>
</section>
<section id="id3">
<h3><span class="section-number">17.3.3. </span>Drawbacks<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Word embeddings <strong>require a large corpus of text data</strong> for training and may not perform well with small datasets.</p></li>
<li><p>They also rely heavily on the quality of the training data and may encode biases present in the corpus.</p></li>
</ul>
</section>
</section>
<section id="glove-global-vectors-for-word-representation">
<h2><span class="section-number">17.4. </span>GloVe (Global Vectors for Word Representation)<a class="headerlink" href="#glove-global-vectors-for-word-representation" title="Link to this heading">#</a></h2>
<p>GloVe (Global Vectors for Word Representation) is a word embedding model introduced by Jeffrey Pennington, Richard Socher, and Christopher D. Manning in 2014. It is an <strong>unsupervised learning</strong> algorithm used for obtaining <strong>vector representations</strong> for words.</p>
<section id="design-purpose">
<h3><span class="section-number">17.4.1. </span>Design Purpose:<a class="headerlink" href="#design-purpose" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>GloVe constructs a co-occurrence matrix where each entry represents the number of times a word co-occurs with another word in the global corpus.</p></li>
<li><p>It then learns word vectors by minimizing a weighted least squares regression objective function that captures the relationship between word vectors and their co-occurrence probabilities.</p></li>
</ul>
</section>
<section id="advantage">
<h3><span class="section-number">17.4.2. </span>Advantage<a class="headerlink" href="#advantage" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Global Co-occurrence Information</strong>: GloVe leverages global <code class="docutils literal notranslate"><span class="pre">word</span> <span class="pre">co-occurrence</span> <span class="pre">statistics</span></code>, capturing richer semantic relationships between words beyond local contexts. It considers the <code class="docutils literal notranslate"><span class="pre">entire</span> <span class="pre">corpus</span></code> when learning word embeddings, which can lead to more informative embeddings.</p></li>
<li><p><strong>Robustness to Out-of-Vocabulary Words</strong>: GloVe can handle out-of-vocabulary words better than Word2Vec and FastText by leveraging global statistics. It can capture semantic similarities between words even if they do not appear frequently in local contexts.</p></li>
<li><p><strong>Interpretability and Generalization</strong>: GloVe embeddings often exhibit good interpretability and generalization capabilities due to their reliance on global co-occurrence statistics. They tend to capture broader semantic relationships and may generalize well to unseen data.</p></li>
</ul>
</section>
<section id="id4">
<h3><span class="section-number">17.4.3. </span>Drawbacks<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Memory Intensive</strong>: GloVe requires storing a large co-occurrence matrix in memory, which can be memory-intensive, especially for very large corpora.</p></li>
<li><p><strong>Training Complexity</strong>: Although GloVe’s objective function is relatively simple compared to neural network-based methods like Word2Vec, training GloVe still involves optimizing a large number of parameters.</p></li>
<li><p><strong>Fixed Context Window</strong>: GloVe typically operates using a fixed context window for calculating co-occurrence statistics. While this window size can be adjusted, it still represents a limitation in capturing contextual information compared to methods that dynamically adjust the context window based on word frequency or document structure.</p></li>
</ul>
</section>
<section id="example-of-co-occurrence-matrix">
<h3><span class="section-number">17.4.4. </span>Example of co-occurrence matrix<a class="headerlink" href="#example-of-co-occurrence-matrix" title="Link to this heading">#</a></h3>
<p>Consider the following small corpus consisting of three sentences:</p>
<ol class="arabic simple">
<li><p>“I like natural language processing.”</p></li>
<li><p>“I enjoy deep learning.”</p></li>
<li><p>“I prefer natural language models.”</p></li>
</ol>
<p>GloVe calculates the co-occurrence matrix by counting how often each word appears in the context of other words within a certain window size. Let’s assume a window size of 1 (i.e., considering adjacent words as context).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>            <span class="o">|</span> <span class="n">I</span> <span class="o">|</span> <span class="n">like</span> <span class="o">|</span> <span class="n">natural</span> <span class="o">|</span> <span class="n">language</span> <span class="o">|</span> <span class="n">processing</span> <span class="o">|</span> <span class="n">enjoy</span> <span class="o">|</span> <span class="n">deep</span> <span class="o">|</span> <span class="n">prefer</span> <span class="o">|</span> <span class="n">models</span> <span class="o">|</span>
<span class="o">-------------------------------------------------------------------------------------------</span>
<span class="n">I</span>           <span class="o">|</span> <span class="mi">0</span> <span class="o">|</span> <span class="mi">1</span>    <span class="o">|</span> <span class="mi">2</span>       <span class="o">|</span> <span class="mi">0</span>        <span class="o">|</span> <span class="mi">0</span>          <span class="o">|</span> <span class="mi">1</span>     <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">1</span>      <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span>
<span class="n">like</span>        <span class="o">|</span> <span class="mi">1</span> <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">1</span>       <span class="o">|</span> <span class="mi">0</span>        <span class="o">|</span> <span class="mi">0</span>          <span class="o">|</span> <span class="mi">0</span>     <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span>
<span class="n">natural</span>     <span class="o">|</span> <span class="mi">2</span> <span class="o">|</span> <span class="mi">1</span>    <span class="o">|</span> <span class="mi">0</span>       <span class="o">|</span> <span class="mi">1</span>        <span class="o">|</span> <span class="mi">1</span>          <span class="o">|</span> <span class="mi">0</span>     <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">1</span>      <span class="o">|</span> <span class="mi">1</span>      <span class="o">|</span>
<span class="n">language</span>    <span class="o">|</span> <span class="mi">0</span> <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">1</span>       <span class="o">|</span> <span class="mi">0</span>        <span class="o">|</span> <span class="mi">1</span>          <span class="o">|</span> <span class="mi">0</span>     <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span> <span class="mi">1</span>      <span class="o">|</span>
<span class="n">processing</span>  <span class="o">|</span> <span class="mi">0</span> <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">1</span>       <span class="o">|</span> <span class="mi">1</span>        <span class="o">|</span> <span class="mi">0</span>          <span class="o">|</span> <span class="mi">0</span>     <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span>
<span class="n">enjoy</span>       <span class="o">|</span> <span class="mi">1</span> <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">0</span>       <span class="o">|</span> <span class="mi">0</span>        <span class="o">|</span> <span class="mi">0</span>          <span class="o">|</span> <span class="mi">0</span>     <span class="o">|</span> <span class="mi">1</span>    <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span>
<span class="n">deep</span>        <span class="o">|</span> <span class="mi">0</span> <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">0</span>       <span class="o">|</span> <span class="mi">0</span>        <span class="o">|</span> <span class="mi">0</span>          <span class="o">|</span> <span class="mi">1</span>     <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span>
<span class="n">prefer</span>      <span class="o">|</span> <span class="mi">1</span> <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">1</span>       <span class="o">|</span> <span class="mi">0</span>        <span class="o">|</span> <span class="mi">0</span>          <span class="o">|</span> <span class="mi">0</span>     <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span> <span class="mi">1</span>      <span class="o">|</span>
<span class="n">models</span>      <span class="o">|</span> <span class="mi">0</span> <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">1</span>       <span class="o">|</span> <span class="mi">1</span>        <span class="o">|</span> <span class="mi">0</span>          <span class="o">|</span> <span class="mi">0</span>     <span class="o">|</span> <span class="mi">0</span>    <span class="o">|</span> <span class="mi">1</span>      <span class="o">|</span> <span class="mi">0</span>      <span class="o">|</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">create_co_occurrence_matrix</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">co_occurrence_matrix</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Tokenize the corpus</span>
    <span class="n">tokenized_corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>

    <span class="c1"># Build vocabulary and co-occurrence matrix</span>
    <span class="k">for</span> <span class="n">tokens</span> <span class="ow">in</span> <span class="n">tokenized_corpus</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
                <span class="n">vocabulary</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                <span class="n">co_occurrence_matrix</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
            
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
                    <span class="n">context_token</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">context_token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">co_occurrence_matrix</span><span class="p">[</span><span class="n">token</span><span class="p">]:</span>
                        <span class="n">co_occurrence_matrix</span><span class="p">[</span><span class="n">token</span><span class="p">][</span><span class="n">context_token</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">co_occurrence_matrix</span><span class="p">[</span><span class="n">token</span><span class="p">][</span><span class="n">context_token</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Convert co-occurrence matrix to numpy array</span>
    <span class="n">vocab_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
    <span class="n">matrix_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
    <span class="n">co_occurrence_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">matrix_size</span><span class="p">,</span> <span class="n">matrix_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token1</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab_list</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">token2</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab_list</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">token2</span> <span class="ow">in</span> <span class="n">co_occurrence_matrix</span><span class="p">[</span><span class="n">token1</span><span class="p">]:</span>
                <span class="n">co_occurrence_array</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">co_occurrence_matrix</span><span class="p">[</span><span class="n">token1</span><span class="p">][</span><span class="n">token2</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">co_occurrence_array</span><span class="p">,</span> <span class="n">vocab_list</span>

<span class="c1"># Example usage</span>
<span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;I like natural language processing.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;I enjoy deep learning.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;I prefer natural language models.&quot;</span>
<span class="p">]</span>

<span class="n">co_occurrence_matrix</span><span class="p">,</span> <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">create_co_occurrence_matrix</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary:&quot;</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Co-occurrence matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">co_occurrence_matrix</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocabulary: [&#39;like&#39;, &#39;processing.&#39;, &#39;deep&#39;, &#39;models.&#39;, &#39;language&#39;, &#39;learning.&#39;, &#39;enjoy&#39;, &#39;I&#39;, &#39;natural&#39;, &#39;prefer&#39;]
Co-occurrence matrix:
[[0 0 0 0 0 0 0 1 1 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 0 0 0 0 1 1 0 0 0]
 [0 0 0 0 1 0 0 0 0 0]
 [0 1 0 1 0 0 0 0 2 0]
 [0 0 1 0 0 0 0 0 0 0]
 [0 0 1 0 0 0 0 1 0 0]
 [1 0 0 0 0 0 1 0 0 1]
 [1 0 0 0 2 0 0 0 0 1]
 [0 0 0 0 0 0 0 1 1 0]]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="contextualized-word-embeddings-elmo-and-bert">
<h2><span class="section-number">17.5. </span>Contextualized word embeddings (ELMo and BERT)<a class="headerlink" href="#contextualized-word-embeddings-elmo-and-bert" title="Link to this heading">#</a></h2>
<section id="elmo-embeddings-from-language-models">
<h3><span class="section-number">17.5.1. </span>ELMo (Embeddings from Language Models)<a class="headerlink" href="#elmo-embeddings-from-language-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://ireneli.eu/2018/12/17/elmo-in-practice/">Reference</a></p></li>
<li><p><strong>Design Purpose:</strong> ELMo was introduced by Matthew Peters and colleagues in 2018. It leverages deep contextualized word representations learned from <code class="docutils literal notranslate"><span class="pre">bidirectional</span> <span class="pre">language</span> <span class="pre">models</span></code> trained on large text corpora.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">ELMo</span></code> generates word embeddings by considering the entire input sentence and capturing the contextual meaning of words.</p></li>
</ul>
</li>
<li><p><strong>Model Architecture</strong>: <code class="docutils literal notranslate"><span class="pre">ELMo</span></code> uses a deep <code class="docutils literal notranslate"><span class="pre">bidirectional</span> <span class="pre">LSTM</span> <span class="pre">(Long</span> <span class="pre">Short-Term</span> <span class="pre">Memory)</span></code> language model to learn contextual representations of words.</p>
<ul>
<li><p>The model is trained to <strong>predict the next word</strong> in a sentence <strong>given both the preceding and following words</strong>, allowing it to capture complex syntactic and semantic dependencies.</p></li>
</ul>
</li>
<li><p><strong>Benefits</strong>: <code class="docutils literal notranslate"><span class="pre">ELMo</span></code> embeddings are context-sensitive and capture fine-grained syntactic and semantic information.</p>
<ul>
<li><p>They can handle polysemy, word sense disambiguation, and syntactic nuances effectively, making them suitable for a wide range of NLP tasks, including sentiment analysis, named entity recognition, and question answering.</p></li>
</ul>
</li>
</ul>
<p><img alt="image.png" src="../../_images/071.png" /></p>
</section>
<section id="bert-bidirectional-encoder-representations-from-transformers">
<h3><span class="section-number">17.5.2. </span>BERT (Bidirectional Encoder Representations from Transformers)<a class="headerlink" href="#bert-bidirectional-encoder-representations-from-transformers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://www.geeksforgeeks.org/understanding-bert-nlp/">Reference</a></p></li>
<li><p><strong>Design Purpose</strong>: <code class="docutils literal notranslate"><span class="pre">BERT</span></code>, introduced by Jacob Devlin and colleagues at Google AI in 2018, is a <code class="docutils literal notranslate"><span class="pre">transformer-based</span> <span class="pre">model</span></code> designed to generate deep contextualized word representations.</p>
<ul>
<li><p>BERT is pretrained on large-scale text corpora using masked language modeling and <strong>next sentence prediction tasks</strong>, enabling it to capture bidirectional context information.</p></li>
</ul>
</li>
<li><p><strong>Model Architecture</strong>: BERT consists of a stack of <strong>transformer encoder layers</strong> that process input sequences bidirectionally.</p>
<ul>
<li><p>During <code class="docutils literal notranslate"><span class="pre">pretraining</span></code>, BERT masks some tokens in the input and predicts them based on the remaining context. This enables the model to learn contextualized embeddings that capture both local and global context information.</p></li>
</ul>
</li>
<li><p><strong>Benefits</strong>: BERT embeddings capture rich contextual information and can model complex linguistic phenomena, including word ambiguity, syntactic structure, and discourse coherence.</p>
<ul>
<li><p>BERT embeddings have achieved state-of-the-art performance on a wide range of NLP tasks, <strong>including question answering, text classification, and natural language inference.</strong></p></li>
</ul>
</li>
</ul>
<p><img alt="image.png" src="../../_images/081.png" /></p>
</section>
</section>
<section id="topic-modeling-lda-lsa">
<h2><span class="section-number">17.6. </span>Topic Modeling (LDA, LSA)<a class="headerlink" href="#topic-modeling-lda-lsa" title="Link to this heading">#</a></h2>
<p>Topic Modeling is a technique used in natural language processing (NLP) to discover the <strong>latent thematic structure</strong> present in a collection of documents.</p>
<ul class="simple">
<li><p>It aims to <strong>identify topics or themes</strong> that pervade a corpus and the distribution of these topics across the documents.</p></li>
<li><p>Two popular methods for topic modeling are <code class="docutils literal notranslate"><span class="pre">Latent</span> <span class="pre">Dirichlet</span> <span class="pre">Allocation</span> <span class="pre">(LDA)</span></code> and <code class="docutils literal notranslate"><span class="pre">Latent</span> <span class="pre">Semantic</span> <span class="pre">Analysis</span> <span class="pre">(LSA)</span></code>.</p></li>
</ul>
<section id="latent-dirichlet-allocation-lda">
<h3><span class="section-number">17.6.1. </span>Latent Dirichlet Allocation (LDA)<a class="headerlink" href="#latent-dirichlet-allocation-lda" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://www.analyticsvidhya.com/blog/2021/06/part-2-topic-modeling-and-latent-dirichlet-allocation-lda-using-gensim-and-sklearn/">Reference</a></p></li>
<li><p><strong>Design Purpose</strong>: <code class="docutils literal notranslate"><span class="pre">LDA</span></code>, introduced by Blei et al. in 2003, is a probabilistic generative model for topic modeling.</p>
<ul>
<li><p>It assumes that documents are represented as a mixture of topics, and <strong>each topic is represented as a distribution over words</strong>.</p></li>
<li><p>LDA aims to infer the underlying topic structure by estimating the distribution of topics in each document and the distribution of words in each topic.</p></li>
</ul>
</li>
<li><p><strong>Model Assumptions</strong>: LDA makes two key assumptions:</p>
<ol class="arabic simple">
<li><p>Each document is a mixture of a small number of topics.</p></li>
<li><p>Each word in a document is generated from one of the topics in the document’s topic mixture.</p></li>
</ol>
</li>
<li><p><strong>Model Inference</strong>: LDA uses Bayesian inference techniques, such as <code class="docutils literal notranslate"><span class="pre">variational</span> <span class="pre">inference</span></code> or <code class="docutils literal notranslate"><span class="pre">Gibbs</span> <span class="pre">sampling</span></code>, to estimate the posterior distribution of topics given the observed documents.</p>
<ul>
<li><p>It iteratively updates the topic assignments of words in each document to find the most likely topic structure.</p></li>
</ul>
</li>
</ul>
<p><img alt="image.png" src="../../_images/091.png" /></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./book\nlp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../cv/Chapter10-CNN.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">16. </span>Convolution Neural Network</p>
      </div>
    </a>
    <a class="right-next"
       href="Chapter10-RNN.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">18. </span>Recurrent Neural Networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-to-matrix-vectors">17.1. Word to Matrix / Vectors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words">17.1.1. Bag-of-Words</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#drawbacks">17.1.1.1. Drawbacks</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-grams">17.1.2. N-grams</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages">17.1.2.1. Advantages</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">17.1.2.2. Drawbacks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mitigation-strategies">17.1.2.3. Mitigation Strategies</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tf-idf">17.2. TF-IDF</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#term-frequency-tf">17.2.1. Term Frequency (TF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse-document-frequency-idf">17.2.2. Inverse Document Frequency (IDF)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">17.2.3. Drawbacks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">17.2.4. Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embeddings-word2vec-fasttext">17.3. Word Embeddings (Word2Vec, FastText)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word2vec">17.3.1. Word2Vec</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fasttext">17.3.2. FastText</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">17.3.3. Drawbacks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#glove-global-vectors-for-word-representation">17.4. GloVe (Global Vectors for Word Representation)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#design-purpose">17.4.1. Design Purpose:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantage">17.4.2. Advantage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">17.4.3. Drawbacks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-co-occurrence-matrix">17.4.4. Example of co-occurrence matrix</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contextualized-word-embeddings-elmo-and-bert">17.5. Contextualized word embeddings (ELMo and BERT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elmo-embeddings-from-language-models">17.5.1. ELMo (Embeddings from Language Models)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bert-bidirectional-encoder-representations-from-transformers">17.5.2. BERT (Bidirectional Encoder Representations from Transformers)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-lda-lsa">17.6. Topic Modeling (LDA, LSA)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-dirichlet-allocation-lda">17.6.1. Latent Dirichlet Allocation (LDA)</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Yalin Yang
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>